{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_cropbox.txt\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from distributed_utilities import *\n",
    "from preprocess_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_fmt = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive s3://mousebrainatlas-rawdata/CSHL_data/MD652 /shared/CSHL_data/MD652 --exclude \"*\" --include \"*.png\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "4.55 seconds.\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD652/MD652_sorted_filenames.txt && mkdir -p /shared/CSHL_data_processed/MD652\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD652/MD652_sorted_filenames.txt /shared/CSHL_data_processed/MD652/MD652_sorted_filenames.txt\n",
      "rm: cannot remove '/shared/CSHL_data_processed/MD652/MD652_elastix_output': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.51 seconds.\n",
      "16 nodes requested, 16 nodes available...Continuing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n",
      "qsub returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 181.380911112 seconds\n",
      "aws s3 cp --recursive /shared/CSHL_data_processed/MD652/MD652_elastix_output s3://mousebrainatlas-data/CSHL_data_processed/MD652/MD652_elastix_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "22.14 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive s3://mousebrainatlas-rawdata/CSHL_data/MD653 /shared/CSHL_data/MD653 --exclude \"*\" --include \"*.png\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "4.72 seconds.\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD653/MD653_sorted_filenames.txt && mkdir -p /shared/CSHL_data_processed/MD653\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD653/MD653_sorted_filenames.txt /shared/CSHL_data_processed/MD653/MD653_sorted_filenames.txt\n",
      "rm: cannot remove '/shared/CSHL_data_processed/MD653/MD653_elastix_output': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.45 seconds.\n",
      "16 nodes requested, 16 nodes available...Continuing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n",
      "qsub returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 397.660309076 seconds\n",
      "aws s3 cp --recursive /shared/CSHL_data_processed/MD653/MD653_elastix_output s3://mousebrainatlas-data/CSHL_data_processed/MD653/MD653_elastix_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "21.23 seconds.\n"
     ]
    }
   ],
   "source": [
    "for stack in ['MD652', 'MD653']:\n",
    "    transfer_data_synced(os.path.join('CSHL_data', stack), \n",
    "                     from_hostname='s3raw', to_hostname='ec2', is_dir=True, include_only='*.'+tb_fmt)\n",
    "\n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_sorted_filenames.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    \n",
    "    ########### Align ###########\n",
    "    \n",
    "    _, sections_to_filenames = DataManager.load_sorted_filenames(stack=stack)\n",
    "    valid_filenames = [fn for fn in sections_to_filenames.values() if not is_invalid(fn=fn)]\n",
    "    \n",
    "    script = os.path.join(REPO_DIR, 'preprocess', 'align_consecutive_v2.py')\n",
    "    input_dir = os.path.join(RAW_DATA_DIR, stack)\n",
    "    output_dir = os.path.join(DATA_DIR, stack, stack + '_elastix_output')\n",
    "    \n",
    "    ! rm -r $output_dir\n",
    "    \n",
    "    t = time.time()\n",
    "    print 'Align...'\n",
    "\n",
    "    run_distributed(\"%(script)s %(stack)s %(input_dir)s %(output_dir)s \\'%%(kwargs_str)s\\' %(fmt)s\" % \\\n",
    "                    {'script': script,\n",
    "                    'stack': stack,\n",
    "                    'input_dir': input_dir,\n",
    "                    'output_dir': output_dir,\n",
    "                    'fmt': tb_fmt},\n",
    "                    kwargs_list=[{'prev_fn': valid_filenames[i-1], 'curr_fn': valid_filenames[i]} for i in range(1, len(valid_filenames))],\n",
    "                    argument_type='list',\n",
    "                   cluster_size=16,\n",
    "                   jobs_per_node=16) # \n",
    "\n",
    "    wait_qsub_complete()\n",
    "\n",
    "    print 'done in', time.time() - t, 'seconds' # 252 seconds\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_elastix_output'), \n",
    "                     from_hostname='ec2', to_hostname='s3', is_dir=True)\n",
    "    \n",
    "    ! tar -C /shared/CSHL_data_processed/{stack}/ -cf /shared/CSHL_data_processed/{stack}/{stack}_elastix_output.tar {stack}_elastix_output/\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_elastix_output.tar'), \n",
    "                     from_hostname='ec2', to_hostname='s3', is_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- download `elastix_output/` to local machine, edit consecutive transforms in local GUI, generate `custom_transforms/` to S3, upload to S3.\n",
    "- determine anchor image, upload `anchor.txt` to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for stack in ['MD652', 'MD653']:\n",
    "# for stack in ['MD642']:\n",
    "    \n",
    "    _, sections_to_filenames = DataManager.load_sorted_filenames(stack=stack)\n",
    "    valid_filenames = [fn for fn in sections_to_filenames.values() if not is_invalid(fn=fn)]\n",
    "    \n",
    "    ########### Compose #############\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_custom_transforms'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=True)\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_anchor.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    \n",
    "    anchor_fn = DataManager.load_anchor_filename(stack=stack)\n",
    "    \n",
    "    script = os.path.join(REPO_DIR, 'preprocess', 'compose_transform_thumbnail_v2.py')\n",
    "    input_dir = os.path.join(DATA_DIR, stack, stack + '_elastix_output')\n",
    "    output_fn = os.path.join(DATA_DIR, stack, '%(stack)s_transformsTo_%(anchor_fn)s.pkl' % \\\n",
    "                                                    dict(stack=stack, anchor_fn=anchor_fn))\n",
    "\n",
    "    ! rm -f *{transformTo}*.pkl\n",
    "    \n",
    "    t = time.time()\n",
    "    print 'Composing transform...'\n",
    "\n",
    "    run_distributed(\"%(script)s %(stack)s %(input_dir)s \\'%%(kwargs_str)s\\' %(anchor_idx)d %(output_fn)s\" % \\\n",
    "                {'stack': stack,\n",
    "                'script': script,\n",
    "                'input_dir': input_dir,\n",
    "                'anchor_idx': valid_filenames.index(anchor_fn),\n",
    "                'output_fn': output_fn},\n",
    "                kwargs_list=[{'filenames': valid_filenames}],\n",
    "                argument_type='list',\n",
    "                   cluster_size=1,\n",
    "                   jobs_per_node=1)\n",
    "\n",
    "    linked_name = os.path.join(DATA_DIR, stack, '%(stack)s_transformsTo_anchor.pkl' % dict(stack=stack))\n",
    "    execute_command('rm -f ' + linked_name)\n",
    "    execute_command('ln -s ' + output_fn + ' ' + linked_name)\n",
    "\n",
    "    print 'done in', time.time() - t, 'seconds'\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack), \n",
    "                     from_hostname='ec2', to_hostname='s3', is_dir=True, include_only='*.pkl')\n",
    "    \n",
    "#     ########## Warp #############\n",
    "    \n",
    "    if stack in all_nissl_stacks:\n",
    "        pad_bg_color = 'white'\n",
    "    else:\n",
    "        pad_bg_color = 'auto'\n",
    "        \n",
    "    script = os.path.join(REPO_DIR, 'preprocess', 'warp_crop_IM_v2.py')\n",
    "    input_dir = os.path.join(RAW_DATA_DIR, stack)\n",
    "    out_dir = os.path.join(DATA_DIR, stack, stack + '_thumbnail_alignedTo_' + anchor_fn)\n",
    "    \n",
    "    ! rm -rf $out_dir\n",
    "    \n",
    "    t = time.time()\n",
    "    print 'Warping...'\n",
    "\n",
    "    transforms_filename = os.path.join(DATA_DIR, stack, '%(stack)s_transformsTo_%(anchor_fn)s.pkl' % \\\n",
    "                                       dict(stack=stack, anchor_fn=anchor_fn))\n",
    "    transforms_to_anchor = pickle.load(open(transforms_filename, 'r'))\n",
    "\n",
    "    if pad_bg_color == 'auto':\n",
    "        run_distributed('%(script)s %(stack)s %(input_dir)s %(out_dir)s %%(transform)s %%(filename)s %%(output_fn)s thumbnail 0 0 2000 1500 %%(pad_bg_color)s' % \\\n",
    "                        {'script': script,\n",
    "                        'stack': stack,\n",
    "                        'input_dir': input_dir,\n",
    "                        'out_dir': out_dir\n",
    "                        },\n",
    "                        kwargs_list=[{'transform': ','.join(map(str, transforms_to_anchor[fn].flatten())),\n",
    "                                    'filename': fn + '.' + tb_fmt,\n",
    "                                    'output_fn': fn + '_thumbnail_alignedTo_' + anchor_fn + '.tif',\n",
    "                                    'pad_bg_color': 'black' if fn.split('-')[1][0] == 'F' else 'white'}\n",
    "                                    for fn in valid_filenames],\n",
    "                        exclude_nodes=[33],\n",
    "                        argument_type='single',\n",
    "                       cluster_size=16,\n",
    "                       jobs_per_node=8)\n",
    "    else:\n",
    "        run_distributed('%(script)s %(stack)s %(input_dir)s %(out_dir)s %%(transform)s %%(filename)s %%(output_fn)s thumbnail 0 0 2000 1500 %(pad_bg_color)s' % \\\n",
    "                        {'script': script,\n",
    "                        'stack': stack,\n",
    "                        'input_dir': input_dir,\n",
    "                        'out_dir': out_dir,\n",
    "                        'pad_bg_color': pad_bg_color},\n",
    "                        kwargs_list=[{'transform': ','.join(map(str, transforms_to_anchor[fn].flatten())),\n",
    "                                    'filename': fn + '.' + tb_fmt,\n",
    "                                    'output_fn': fn + '_thumbnail_alignedTo_' + anchor_fn + '.tif'}\n",
    "                                    for fn in valid_filenames],\n",
    "                        exclude_nodes=[33],\n",
    "                        argument_type='single',\n",
    "                       cluster_size=16,\n",
    "                       jobs_per_node=8)\n",
    "        \n",
    "    wait_qsub_complete()\n",
    "\n",
    "    print 'done in', time.time() - t, 'seconds' # 266 seconds (aws)\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_thumbnail_alignedTo_' + anchor_fn),\n",
    "                    from_hostname='ec2',\n",
    "                    to_hostname='s3',\n",
    "                    is_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Go to local GUI, download aligned images to check correctness\n",
    "- Place cropbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD642/MD642_cropbox.txt && mkdir -p /shared/CSHL_data_processed/MD642\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD642/MD642_cropbox.txt /shared/CSHL_data_processed/MD642/MD642_cropbox.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "Child returned 0\n",
      "0.45 seconds.\n",
      "Child returned 0\n",
      "cropping thumbnail..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p /shared/CSHL_data_processed/MD642/MD642_thumbnail_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped\n",
      "mogrify -set filename:name %t -crop 897x487+576+313 -write \"/shared/CSHL_data_processed/MD642/MD642_thumbnail_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped/%[filename:name]_cropped.tif\" /shared/CSHL_data_processed/MD642/MD642_thumbnail_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157/*.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "done in 40.003121 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive /shared/CSHL_data_processed/MD642/MD642_thumbnail_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped s3://mousebrainatlas-data/CSHL_data_processed/MD642/MD642_thumbnail_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "3.27 seconds.\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD652/MD652_cropbox.txt && mkdir -p /shared/CSHL_data_processed/MD652\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD652/MD652_cropbox.txt /shared/CSHL_data_processed/MD652/MD652_cropbox.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.46 seconds.\n",
      "Child returned 0\n",
      "cropping thumbnail..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p /shared/CSHL_data_processed/MD652/MD652_thumbnail_alignedTo_MD652-F45-2016.12.17-05.56.31_MD652_2_0134_cropped\n",
      "mogrify -set filename:name %t -crop 710x486+639+324 -write \"/shared/CSHL_data_processed/MD652/MD652_thumbnail_alignedTo_MD652-F45-2016.12.17-05.56.31_MD652_2_0134_cropped/%[filename:name]_cropped.tif\" /shared/CSHL_data_processed/MD652/MD652_thumbnail_alignedTo_MD652-F45-2016.12.17-05.56.31_MD652_2_0134/*.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "done in 39.959515 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive /shared/CSHL_data_processed/MD652/MD652_thumbnail_alignedTo_MD652-F45-2016.12.17-05.56.31_MD652_2_0134_cropped s3://mousebrainatlas-data/CSHL_data_processed/MD652/MD652_thumbnail_alignedTo_MD652-F45-2016.12.17-05.56.31_MD652_2_0134_cropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "3.55 seconds.\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD653/MD653_cropbox.txt && mkdir -p /shared/CSHL_data_processed/MD653\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD653/MD653_cropbox.txt /shared/CSHL_data_processed/MD653/MD653_cropbox.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.42 seconds.\n",
      "Child returned 0\n",
      "cropping thumbnail..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p /shared/CSHL_data_processed/MD653/MD653_thumbnail_alignedTo_MD653-F64-2016.12.21-12.03.55_MD653_2_0191_cropped\n",
      "mogrify -set filename:name %t -crop 802x525+633+187 -write \"/shared/CSHL_data_processed/MD653/MD653_thumbnail_alignedTo_MD653-F64-2016.12.21-12.03.55_MD653_2_0191_cropped/%[filename:name]_cropped.tif\" /shared/CSHL_data_processed/MD653/MD653_thumbnail_alignedTo_MD653-F64-2016.12.21-12.03.55_MD653_2_0191/*.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "done in 41.535591 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive /shared/CSHL_data_processed/MD653/MD653_thumbnail_alignedTo_MD653-F64-2016.12.21-12.03.55_MD653_2_0191_cropped s3://mousebrainatlas-data/CSHL_data_processed/MD653/MD653_thumbnail_alignedTo_MD653-F64-2016.12.21-12.03.55_MD653_2_0191_cropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "3.81 seconds.\n"
     ]
    }
   ],
   "source": [
    "for stack in ['MD642', 'MD652', 'MD653']:\n",
    "\n",
    "    _, sections_to_filenames = DataManager.load_sorted_filenames(stack=stack)\n",
    "    valid_filenames = [fn for fn in sections_to_filenames.values() if not is_invalid(fn=fn)]\n",
    "    \n",
    "    anchor_fn = DataManager.load_anchor_filename(stack=stack)\n",
    "\n",
    "    ############ Crop ###############\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_cropbox.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    \n",
    "    xmin, xmax, ymin, ymax, first_sec, last_sec = DataManager.load_cropbox(stack=stack)\n",
    "    w = xmax + 1 - xmin\n",
    "    h = ymax + 1 - ymin\n",
    "    x = xmin\n",
    "    y = ymin\n",
    "    \n",
    "    first_fn = sections_to_filenames[first_sec]\n",
    "    last_fn = sections_to_filenames[last_sec]\n",
    "\n",
    "    first_idx_among_valid = valid_filenames.index(first_fn)\n",
    "    last_idx_among_valid = valid_filenames.index(last_fn)\n",
    "    \n",
    "    input_dir = os.path.join(DATA_DIR, stack, \"%(stack)s_thumbnail_alignedTo_%(anchor_fn)s\" % \\\n",
    "                           {'stack': stack, 'anchor_fn': anchor_fn})\n",
    "    output_dir = os.path.join(DATA_DIR, stack, \"%(stack)s_thumbnail_alignedTo_%(anchor_fn)s_cropped\" % \\\n",
    "                               {'stack': stack, 'anchor_fn': anchor_fn})\n",
    "    execute_command('mkdir -p ' + output_dir)\n",
    "    \n",
    "    t = time.time()\n",
    "    sys.stderr.write('cropping thumbnail...')\n",
    "\n",
    "    execute_command('mogrify -set filename:name %%t -crop %(w)dx%(h)d+%(x)d+%(y)d -write \"%(output_dir)s/%%[filename:name]_cropped.tif\" %(input_dir)s/*.tif' % \\\n",
    "        {'input_dir': input_dir,\n",
    "         'output_dir': output_dir,\n",
    "        'w':w, 'h':h, 'x':x, 'y':y})\n",
    "\n",
    "    sys.stderr.write('done in %f seconds\\n' % (time.time() - t)) # 100 seconds\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_thumbnail_alignedTo_' + anchor_fn + '_cropped'),\n",
    "                    from_hostname='ec2',\n",
    "                    to_hostname='s3',\n",
    "                    is_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "expanding...16 nodes requested, 16 nodes available...Continuing\n",
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n",
      "qsub returned.\n",
      "16 nodes requested, 16 nodes available...Continuing\n",
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n",
      "qsub returned.\n",
      "16 nodes requested, 16 nodes available...Continuing\n",
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n"
     ]
    }
   ],
   "source": [
    "for stack in ['MD652', 'MD653']:\n",
    "# for stack in ['MD642']:\n",
    "\n",
    "    _, sections_to_filenames = DataManager.load_sorted_filenames(stack=stack)\n",
    "    valid_filenames = [fn for fn in sections_to_filenames.values() if not is_invalid(fn=fn)]\n",
    "    \n",
    "    xmin, xmax, ymin, ymax, first_sec, last_sec = DataManager.load_cropbox(stack=stack)\n",
    "    w = xmax + 1 - xmin\n",
    "    h = ymax + 1 - ymin\n",
    "    x = xmin\n",
    "    y = ymin\n",
    "    \n",
    "    first_fn = sections_to_filenames[first_sec]\n",
    "    last_fn = sections_to_filenames[last_sec]\n",
    "\n",
    "    first_idx_among_valid = valid_filenames.index(first_fn)\n",
    "    last_idx_among_valid = valid_filenames.index(last_fn)\n",
    "    \n",
    "    ########## Expand lossless JPEG2000 ##########\n",
    "\n",
    "    t = time.time()\n",
    "    sys.stderr.write('expanding...')\n",
    "\n",
    "    output_dir = os.path.join('/scratch', 'CSHL_data_processed', stack, stack + '_lossless_tif')\n",
    "    input_dir = os.path.join('/scratch', 'CSHL_data', stack)\n",
    "    \n",
    "    \n",
    "    run_distributed('rm -r %(input_dir)s' % {'input_dir': input_dir},\n",
    "                    argument_type='single',\n",
    "                   cluster_size=16)\n",
    "    wait_qsub_complete()\n",
    "\n",
    "    run_distributed('rm -rf %(output_dir)s; mkdir -p %(output_dir)s' % {'output_dir': output_dir},\n",
    "                        argument_type='single',\n",
    "                       cluster_size=16)\n",
    "    wait_qsub_complete()\n",
    "    \n",
    "\n",
    "    filenames_to_expand = [fn for fn in valid_filenames[first_idx_among_valid:last_idx_among_valid+1] \n",
    "                           if not os.path.exists(os.path.join(input_dir, fn + '_lossless.tif'))]\n",
    "\n",
    "    download_cmd = 'aws s3 cp s3://mousebrainatlas-rawdata/%(s3_dir)s/%%(fn)s_lossless.jp2 %(input_dir)s/ &&' % \\\n",
    "            {'s3_dir': relative_to_ec2(input_dir, '/scratch'), 'input_dir': input_dir}\n",
    "        \n",
    "    expand_cmd = 'export LD_LIBRARY_PATH=/home/ubuntu/KDU79_Demo_Apps_for_Linux-x86-64_170108:$LD_LIBRARY_PATH; %(kdu_bin)s -i %(input_dir)s/%%(fn)s_lossless.jp2 -o %(output_dir)s/%%(fn)s_lossless.tif &&' % \\\n",
    "                    {'kdu_bin': KDU_EXPAND_BIN,\n",
    "                     'output_dir': output_dir,\n",
    "                    'input_dir': input_dir}\n",
    "\n",
    "    upload_cmd = 'aws s3 cp %(output_dir)s/%%(fn)s_lossless.tif s3://mousebrainatlas-data/%(s3_dir)s/ &&' % \\\n",
    "            {'output_dir': output_dir, 's3_dir': relative_to_ec2(output_dir, '/scratch')}\n",
    "\n",
    "    cleanup_cmd = 'rm -f %(input_dir)s/%%(fn)s_lossless.jp2; rm -f %(output_dir)s/%%(fn)s_lossless.tif' % \\\n",
    "            {'input_dir': input_dir, 'output_dir': output_dir}\n",
    "        \n",
    "    run_distributed(download_cmd + expand_cmd + upload_cmd + cleanup_cmd,\n",
    "                     kwargs_list={'fn': filenames_to_expand},\n",
    "                    argument_type='single',\n",
    "                   cluster_size=16)\n",
    "\n",
    "    wait_qsub_complete()\n",
    "\n",
    "    sys.stderr.write('done in %f seconds\\n' % (time.time() - t)) # 1400 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warp and crop lossless...Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD642/MD642_sorted_filenames.txt && mkdir -p /shared/CSHL_data_processed/MD642\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD642/MD642_sorted_filenames.txt /shared/CSHL_data_processed/MD642/MD642_sorted_filenames.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.43 seconds.\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD642/MD642_anchor.txt && mkdir -p /shared/CSHL_data_processed/MD642\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD642/MD642_anchor.txt /shared/CSHL_data_processed/MD642/MD642_anchor.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.42 seconds.\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD642/MD642_cropbox.txt && mkdir -p /shared/CSHL_data_processed/MD642\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD642/MD642_cropbox.txt /shared/CSHL_data_processed/MD642/MD642_cropbox.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.42 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive s3://mousebrainatlas-data/CSHL_data_processed/MD642 /shared/CSHL_data_processed/MD642 --exclude \"*\" --include \"*.pkl\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "2.96 seconds.\n",
      "16 nodes requested, 16 nodes available...Continuing\n",
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n",
      "qsub returned.\n",
      "16 nodes requested, 16 nodes available...Continuing\n",
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n",
      "qsub returned.\n",
      "warping and cropping lossless...16 nodes requested, 16 nodes available...Continuing\n",
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n"
     ]
    }
   ],
   "source": [
    "for stack in ['MD642', 'MD652', 'MD653']:\n",
    "# for stack in ['MD642']:\n",
    "    \n",
    "    ############# Load prerequisites ###############\n",
    "    \n",
    "    t = time.time()\n",
    "    sys.stderr.write('warp and crop lossless...')\n",
    "    \n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_sorted_filenames.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    _, sections_to_filenames = DataManager.load_sorted_filenames(stack=stack)\n",
    "    valid_filenames = [fn for fn in sections_to_filenames.values() if not is_invalid(fn=fn)]\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_anchor.txt'), \n",
    "                 from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    anchor_fn = DataManager.load_anchor_filename(stack=stack)\n",
    "\n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_cropbox.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    xmin, xmax, ymin, ymax, first_sec, last_sec = DataManager.load_cropbox(stack=stack)\n",
    "    w = xmax + 1 - xmin\n",
    "    h = ymax + 1 - ymin\n",
    "    x = xmin\n",
    "    y = ymin\n",
    "    \n",
    "    first_fn = sections_to_filenames[first_sec]\n",
    "    last_fn = sections_to_filenames[last_sec]\n",
    "\n",
    "    first_idx_among_valid = valid_filenames.index(first_fn)\n",
    "    last_idx_among_valid = valid_filenames.index(last_fn)\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack), \n",
    "                 from_hostname='s3', to_hostname='ec2', is_dir=True, include_only='*.pkl')\n",
    "    \n",
    "    tf_filepath = os.path.join(DATA_DIR, stack, '%(stack)s_transformsTo_anchor.pkl' % {'stack':stack})\n",
    "    tfs = pickle.load(open(tf_filepath, 'r'))\n",
    "    \n",
    "    ############# Warp and crop lossless ###############\n",
    "    \n",
    "    script_fp = os.path.join(REPO_DIR, 'preprocess', 'warp_crop_IM_v2.py')\n",
    "    \n",
    "    input_dir = os.path.join('/scratch', 'CSHL_data_processed', stack, stack + '_lossless_tif')\n",
    "    output_dir = os.path.join('/scratch', 'CSHL_data_processed', stack, stack + '_lossless_alignedTo_' + anchor_fn + '_cropped')\n",
    "    \n",
    "    run_distributed('rm -r %(input_dir)s' % {'input_dir': input_dir},\n",
    "                    argument_type='single',\n",
    "                   cluster_size=16)\n",
    "    wait_qsub_complete()\n",
    "\n",
    "    run_distributed('rm -rf %(output_dir)s; mkdir -p %(output_dir)s' % {'output_dir': output_dir},\n",
    "                        argument_type='single',\n",
    "                       cluster_size=16)\n",
    "    wait_qsub_complete()\n",
    "    \n",
    "    if stack in all_nissl_stacks:\n",
    "        pad_bg_color = 'white'\n",
    "    else:\n",
    "        pad_bg_color = 'auto'\n",
    "    \n",
    "    t = time.time()\n",
    "    sys.stderr.write('warping and cropping lossless...')\n",
    "    \n",
    "    download_cmd = 'aws s3 cp s3://mousebrainatlas-data/%(s3_dir)s/%%(filename)s %(input_dir)s/' % \\\n",
    "            {'s3_dir': relative_to_ec2(input_dir, '/scratch'), 'input_dir': input_dir}\n",
    "    \n",
    "    if pad_bg_color == 'auto':\n",
    "        work_cmd = '%(script_path)s %(stack)s %(input_dir)s %(output_dir)s %%(transform)s %%(filename)s %%(output_fn)s lossless %(x)d %(y)d %(w)d %(h)d %%(pad_bg_color)s'%\\\n",
    "                    {'script_path': script_fp,\n",
    "                    'stack': stack,\n",
    "                    'input_dir': input_dir,\n",
    "                    'output_dir': output_dir,\n",
    "                    'x': x,\n",
    "                    'y': y,\n",
    "                    'w': w,\n",
    "                    'h': h}\n",
    "    else:\n",
    "        work_cmd = '%(script_path)s %(stack)s %(input_dir)s %(output_dir)s %%(transform)s %%(filename)s %%(output_fn)s lossless %(x)d %(y)d %(w)d %(h)d %(pad_bg_color)s'%\\\n",
    "                    {'script_path': script_fp,\n",
    "                    'stack': stack,\n",
    "                    'input_dir': input_dir,\n",
    "                    'output_dir': output_dir,\n",
    "                    'pad_bg_color': pad_bg_color,\n",
    "                    'x': x,\n",
    "                    'y': y,\n",
    "                    'w': w,\n",
    "                    'h': h}\n",
    "    \n",
    "    upload_cmd = 'aws s3 cp %(output_dir)s/%%(output_fn)s s3://mousebrainatlas-data/%(s3_dir)s/' % \\\n",
    "            {'output_dir': output_dir, 's3_dir': relative_to_ec2(output_dir, '/scratch'), 'anchor_fn': anchor_fn}\n",
    "        \n",
    "    cleanup_cmd = 'rm -f %(input_dir)s/%%(filename)s; rm -f %(output_dir)s/%%(output_fn)s' % \\\n",
    "            {'input_dir': input_dir, 'output_dir': output_dir}\n",
    "    \n",
    "    if pad_bg_color == 'auto':\n",
    "        run_distributed('&&'.join([download_cmd, work_cmd, upload_cmd, cleanup_cmd]),\n",
    "                        kwargs_list=[{'transform': ','.join(map(str, tfs[fn].flatten())),\n",
    "                                    'filename': fn + '_lossless.tif',\n",
    "                                    'output_fn': fn + '_lossless_alignedTo_' + anchor_fn + '_cropped.tif',\n",
    "                                    'pad_bg_color': 'black' if fn.split('-')[1][0] == 'F' else 'white'}\n",
    "                                    for fn in valid_filenames[first_idx_among_valid:last_idx_among_valid+1]],                        \n",
    "                        argument_type='single',\n",
    "                       cluster_size=16,\n",
    "                       jobs_per_node=2)\n",
    "    else:\n",
    "        run_distributed('&&'.join([download_cmd, work_cmd, upload_cmd, cleanup_cmd]),\n",
    "                     kwargs_list=[{'transform': ','.join(map(str, tfs[fn].flatten())),\n",
    "                                    'filename': fn + '_lossless.tif',\n",
    "                                    'output_fn': fn + '_lossless_alignedTo_' + anchor_fn + '_cropped.tif'}\n",
    "                                    for fn in valid_filenames[first_idx_among_valid:last_idx_among_valid+1]],\n",
    "                    argument_type='single',\n",
    "                       cluster_size=16,\n",
    "                       jobs_per_node=2)\n",
    "\n",
    "    wait_qsub_complete()\n",
    "    \n",
    "    sys.stderr.write('done in %f seconds\\n' % (time.time() - t)) # 1400 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for stack in ['MD642', 'MD652', 'MD653']:\n",
    "    \n",
    "    ############# Generate mask ###############\n",
    "    \n",
    "    t = time.time()\n",
    "    sys.stderr.write('warp and crop lossless...')\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_sorted_filenames.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    _, sections_to_filenames = DataManager.load_sorted_filenames(stack=stack)\n",
    "    valid_filenames = [fn for fn in sections_to_filenames.values() if not is_invalid(fn=fn)]\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_anchor.txt'), \n",
    "                 from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    anchor_fn = DataManager.load_anchor_filename(stack=stack)\n",
    "\n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_cropbox.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    xmin, xmax, ymin, ymax, first_sec, last_sec = DataManager.load_cropbox(stack=stack)\n",
    "    w = xmax + 1 - xmin\n",
    "    h = ymax + 1 - ymin\n",
    "    x = xmin\n",
    "    y = ymin\n",
    "    \n",
    "    first_fn = sections_to_filenames[first_sec]\n",
    "    last_fn = sections_to_filenames[last_sec]\n",
    "\n",
    "    first_idx_among_valid = valid_filenames.index(first_fn)\n",
    "    last_idx_among_valid = valid_filenames.index(last_fn)\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack), \n",
    "                 from_hostname='s3', to_hostname='ec2', is_dir=True, include_only='*.pkl')\n",
    "    \n",
    "    tf_filepath = os.path.join(DATA_DIR, stack, '%(stack)s_transformsTo_anchor.pkl' % {'stack':stack})\n",
    "    tfs = pickle.load(open(tf_filepath, 'r'))\n",
    "    \n",
    "    ############# Warp and crop lossless ###############\n",
    "    \n",
    "    script_fp = os.path.join(REPO_DIR, 'preprocess', 'warp_crop_IM_v2.py')\n",
    "    \n",
    "    input_dir = os.path.join('/scratch', 'CSHL_data_processed', stack, stack + '_lossless_tif')\n",
    "    output_dir = os.path.join('/scratch', 'CSHL_data_processed', stack, stack + '_lossless_alignedTo_' + anchor_fn + '_cropped')\n",
    "    \n",
    "    run_distributed('rm -r %(input_dir)s' % {'input_dir': input_dir},\n",
    "                    argument_type='single',\n",
    "                   cluster_size=16)\n",
    "    wait_qsub_complete()\n",
    "\n",
    "    run_distributed('rm -rf %(output_dir)s; mkdir -p %(output_dir)s' % {'output_dir': output_dir},\n",
    "                        argument_type='single',\n",
    "                       cluster_size=16)\n",
    "    wait_qsub_complete()\n",
    "    \n",
    "    if stack in all_nissl_stacks:\n",
    "        pad_bg_color = 'white'\n",
    "    else:\n",
    "        pad_bg_color = 'auto'\n",
    "    \n",
    "    t = time.time()\n",
    "    sys.stderr.write('warping and cropping lossless...')\n",
    "    \n",
    "    download_cmd = 'aws s3 cp s3://mousebrainatlas-data/%(s3_dir)s/%%(filename)s %(input_dir)s/' % \\\n",
    "            {'s3_dir': relative_to_ec2(input_dir, '/scratch'), 'input_dir': input_dir}\n",
    "    \n",
    "    if pad_bg_color == 'auto':\n",
    "        work_cmd = '%(script_path)s %(stack)s %(input_dir)s %(output_dir)s %%(transform)s %%(filename)s %%(output_fn)s lossless %(x)d %(y)d %(w)d %(h)d %%(pad_bg_color)s'%\\\n",
    "                    {'script_path': script_fp,\n",
    "                    'stack': stack,\n",
    "                    'input_dir': input_dir,\n",
    "                    'output_dir': output_dir,\n",
    "                    'x': x,\n",
    "                    'y': y,\n",
    "                    'w': w,\n",
    "                    'h': h}\n",
    "    else:\n",
    "        work_cmd = '%(script_path)s %(stack)s %(input_dir)s %(output_dir)s %%(transform)s %%(filename)s %%(output_fn)s lossless %(x)d %(y)d %(w)d %(h)d %(pad_bg_color)s'%\\\n",
    "                    {'script_path': script_fp,\n",
    "                    'stack': stack,\n",
    "                    'input_dir': input_dir,\n",
    "                    'output_dir': output_dir,\n",
    "                    'pad_bg_color': pad_bg_color,\n",
    "                    'x': x,\n",
    "                    'y': y,\n",
    "                    'w': w,\n",
    "                    'h': h}\n",
    "    \n",
    "    upload_cmd = 'aws s3 cp %(output_dir)s/%%(output_fn)s s3://mousebrainatlas-data/%(s3_dir)s/' % \\\n",
    "            {'output_dir': output_dir, 's3_dir': relative_to_ec2(output_dir, '/scratch'), 'anchor_fn': anchor_fn}\n",
    "        \n",
    "    cleanup_cmd = 'rm -f %(input_dir)s/%%(filename)s; rm -f %(output_dir)s/%%(output_fn)s' % \\\n",
    "            {'input_dir': input_dir, 'output_dir': output_dir}\n",
    "    \n",
    "    if pad_bg_color == 'auto':\n",
    "        run_distributed('&&'.join([download_cmd, work_cmd, upload_cmd, cleanup_cmd]),\n",
    "                        kwargs_list=[{'transform': ','.join(map(str, tfs[fn].flatten())),\n",
    "                                    'filename': fn + '_lossless.tif',\n",
    "                                    'output_fn': fn + '_lossless_alignedTo_' + anchor_fn + '_cropped.tif',\n",
    "                                    'pad_bg_color': 'black' if fn.split('-')[1][0] == 'F' else 'white'}\n",
    "                                    for fn in valid_filenames[first_idx_among_valid:last_idx_among_valid+1]],                        \n",
    "                        argument_type='single',\n",
    "                       cluster_size=16,\n",
    "                       jobs_per_node=2)\n",
    "    else:\n",
    "        run_distributed('&&'.join([download_cmd, work_cmd, upload_cmd, cleanup_cmd]),\n",
    "                     kwargs_list=[{'transform': ','.join(map(str, tfs[fn].flatten())),\n",
    "                                    'filename': fn + '_lossless.tif',\n",
    "                                    'output_fn': fn + '_lossless_alignedTo_' + anchor_fn + '_cropped.tif'}\n",
    "                                    for fn in valid_filenames[first_idx_among_valid:last_idx_among_valid+1]],\n",
    "                    argument_type='single',\n",
    "                       cluster_size=16,\n",
    "                       jobs_per_node=2)\n",
    "\n",
    "    wait_qsub_complete()\n",
    "    \n",
    "    sys.stderr.write('done in %f seconds\\n' % (time.time() - t)) # 1400 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive s3://mousebrainatlas-rawdata/CSHL_data/MD652 /shared/CSHL_data/MD652 --exclude \"*\" --include \"*.png\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "4.63 seconds.\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD652/MD652_sorted_filenames.txt && mkdir -p /shared/CSHL_data_processed/MD652\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD652/MD652_sorted_filenames.txt /shared/CSHL_data_processed/MD652/MD652_sorted_filenames.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.45 seconds.\n",
      "All nodes are ready.\n",
      "Wait for SGE to know all nodes (timeout in 300 seconds)...\n",
      "All nodes are ready.\n",
      "16 nodes requested, 16 nodes available...Continuing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating thumbnail mask..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jobs submitted. Use wait_qsub_complete() to check if they finish.\n",
      "qsub returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done in 533.41121006 seconds\n",
      "aws s3 cp --recursive /shared/CSHL_data_processed/MD652/MD652_submasks s3://mousebrainatlas-data/CSHL_data_processed/MD652/MD652_submasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "6.57 seconds.\n"
     ]
    }
   ],
   "source": [
    "# for stack in ['MD642', 'MD652', 'MD653']:\n",
    "for stack in ['MD652']:\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data', stack), \n",
    "                         from_hostname='s3raw', to_hostname='ec2', is_dir=True, include_only='*.'+tb_fmt)\n",
    "    \n",
    "    transfer_data_synced(os.path.join('CSHL_data_processed', stack, stack + '_sorted_filenames.txt'), \n",
    "                     from_hostname='s3', to_hostname='ec2', is_dir=False)\n",
    "    _, sections_to_filenames = DataManager.load_sorted_filenames(stack=stack)\n",
    "    valid_filenames = [fn for fn in sections_to_filenames.values() if not is_invalid(fn=fn)]\n",
    "    \n",
    "    script = os.path.join(REPO_DIR, 'preprocess', 'generate_thumbnail_masks_v4.py')\n",
    "    input_dir = os.path.join(RAW_DATA_DIR, stack)\n",
    "    output_dir = create_if_not_exists(os.path.join(DATA_DIR, stack, stack + '_submasks'))\n",
    "    ! rm -f output_dir/*\n",
    "    \n",
    "    t = time.time()\n",
    "    print 'Generating thumbnail mask...',\n",
    "\n",
    "    wait_num_nodes(16)\n",
    "\n",
    "    run_distributed(command='%(script_path)s %(stack)s %(input_dir)s \\'%%(filenames)s\\' %(output_dir)s' % \\\n",
    "                    {'script_path': script,\n",
    "                    'stack': stack,\n",
    "                    'input_dir': input_dir,\n",
    "                    'output_dir': output_dir},\n",
    "                    kwargs_list=dict(filenames=valid_filenames),\n",
    "                    argument_type='list2',\n",
    "                   cluster_size=16,\n",
    "                   jobs_per_node=1)\n",
    "\n",
    "    wait_qsub_complete()\n",
    "\n",
    "    print 'done in', time.time() - t, 'seconds' # 300s (aws)\n",
    "    \n",
    "    transfer_data_synced(relative_to_ec2(output_dir),\n",
    "                    from_hostname='ec2',\n",
    "                    to_hostname='s3',\n",
    "                    is_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm ~/*.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 part(s) with ... file(s) remaining\r",
      "\r\n",
      "Completed 1 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 2 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 3 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 4 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 5 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 6 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 7 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 8 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 9 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 10 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 11 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 12 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 13 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 14 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 15 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 16 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 17 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 18 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 19 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 20 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 21 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 22 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 23 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 24 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 25 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 26 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 27 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 28 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 29 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 30 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 31 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 32 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 33 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 34 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 35 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 36 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 37 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 38 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 39 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 40 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 41 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 42 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 43 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 44 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 45 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 46 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 47 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 48 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 49 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 50 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 51 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 52 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 53 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 54 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 55 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 56 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 57 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 58 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 59 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 60 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 61 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 62 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 63 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 64 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 65 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 66 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 67 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 68 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 69 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 70 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 71 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 72 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 73 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 74 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 75 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 76 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 77 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 78 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 79 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 80 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 81 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 82 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 83 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 84 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 85 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 86 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 87 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 88 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 89 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 90 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 91 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 92 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 93 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 94 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 95 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 96 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 97 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 98 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 99 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 100 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 101 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 102 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 103 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 104 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 105 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 106 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 107 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 108 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 109 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 110 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 111 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 112 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 113 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 114 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 115 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 116 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 117 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 118 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 119 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 120 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 121 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 122 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 123 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 124 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 125 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 126 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 127 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 128 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 129 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 130 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 131 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 132 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 133 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 134 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 135 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 136 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 137 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 138 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 139 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 140 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 141 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 142 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 143 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 144 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 145 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 146 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 147 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 148 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 149 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 150 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 151 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 152 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 153 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 154 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 155 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 156 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 157 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 158 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 159 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 160 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 161 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 162 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 163 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 164 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 165 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 166 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 167 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 168 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 169 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 170 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 171 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 172 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 173 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 174 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 175 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 176 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 177 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 178 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 179 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 180 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 181 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 182 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 183 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 184 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 185 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 186 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 187 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 188 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 189 of 190 part(s) with 1 file(s) remaining\r",
      "Completed 190 of 190 part(s) with 1 file(s) remaining\r",
      "download: s3://mousebrainatlas-data/CSHL_data_processed/MD642/MD642_lossless_tif/MD642-N14-2017.01.18-16.25.58_MD642_3_0042_lossless.tif to ../../scratch/CSHL_data_processed/MD642/MD642_lossless_tif/MD642-N14-2017.01.18-16.25.58_MD642_3_0042_lossless.tif\r\n"
     ]
    }
   ],
   "source": [
    "! tail ~/stdout_2_0.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/scratch/CSHL_data_processed/MD642/MD642_lossless_tif': No such file or directory\r\n",
      "An error occurred (404) when calling the HeadObject operation: Key \"CSHL_data_processed/MD642/MD642_lossless_tif/MD642-N14-2017.01.18-16.25.58_MD642_3_0042_lossless.tif\" does not exist\r\n",
      "rm: cannot remove '/scratch/CSHL_data_processed/MD642/MD642_lossless_tif': No such file or directory\r\n",
      "usage: warp_crop_IM_v2.py [-h]\r\n",
      "                          stack_name input_dir output_dir transform filename\r\n",
      "                          output_fn suffix x y w h background_color\r\n",
      "warp_crop_IM_v2.py: error: unrecognized arguments: s3 cp /scratch/CSHL_data_processed/MD642/MD642_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped/MD642-N14-2017.01.18-16.25.58_MD642_3_0042_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped.tif s3://mousebrainatlas-data/CSHL_data_processed/MD642/MD642_lossless_alignedTo_MD642-F53-2017.01.14-12.23.43_MD642_1_0157_cropped/\r\n"
     ]
    }
   ],
   "source": [
    "! tail ~/stderr_2_0.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
