{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Eliminate small blobs, does not eliminate blobs with high compactness value\n",
    "2. Collect typical blob by local blob matching\n",
    "3. Collect false typical blob by matching blobs with blobs far away in the same section \n",
    "    - Set a cut off radius and compare this blob with all the blobs outside this radius on the same section\n",
    "    - Collect the best matched pair. Or alternatively, randomly sample the outside blob and form a pair\n",
    "4. Run over the entire brain\n",
    "    - No need in the first trail,where we just need to get a large enough region. Let's look at the whole image first. \n",
    "5. Train the average perceptron with the same features\n",
    "6. Train the average perceptron with more features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n",
      "No vtk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot import mxnet.\n",
      "Folder already exists: /shared/MouseBrainAtlasXiang/XJ/Output/Collect_typical_blobs/"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, time, collections\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'cells'))\n",
    "# sys.path.append(os.path.join(os.environ['REPO_DIR'], 'annotation'))\n",
    "# sys.path.append(os.path.join(os.environ['REPO_DIR']bb, 'learning'))\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import skimage\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.morphology import watershed\n",
    "from skimage.measure import regionprops, label, find_contours\n",
    "from skimage.color import rgb2hsv\n",
    "from scipy import ndimage as ndi\n",
    "import pandas\n",
    "from scipy.signal import argrelmax\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from annotation_utilities import *\n",
    "from registration_utilities import *\n",
    "from learning_utilities import *\n",
    "from data_manager import *\n",
    "from utilities2015 import *\n",
    "from cell_utilities import *\n",
    "from metadata import *\n",
    "from distributed_utilities import download_from_s3\n",
    "\n",
    "save_folder_path = '/shared/MouseBrainAtlasXiang/XJ/Output/Collect_typical_blobs/'\n",
    "from xj_utilities import *\n",
    "fun_create_folder(save_folder_path=save_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scan_parameters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scan_parameters['stack'] = 'MD589'\n",
    "scan_parameters['patch_size'] = 448\n",
    "scan_parameters['patch_half_size'] = scan_parameters['patch_size']/2\n",
    "scan_parameters['stride'] = 112\n",
    "\n",
    "scan_parameters['section_limits'] = metadata_cache['section_limits'][scan_parameters['stack']]\n",
    "scan_parameters['o_crop'] = True\n",
    "scan_parameters['o_clear_border'] = True\n",
    "scan_parameters['o_relabel'] = True\n",
    "scan_parameters['oriImL1'], scan_parameters['oriImL0'] = metadata_cache['image_shape'][scan_parameters['stack']]\n",
    "scan_parameters['scan_section_range'] = 1\n",
    "\n",
    "scan_parameters['prop'] = ['centroid','eccentricity','area','orientation','moments_hu','bbox','equivalent_diameter','perimeter','compactness','label','major_axis_length','minor_axis_length']\n",
    "scan_parameters['prop_for_comparison'] = ['area', 'eccentricity']\n",
    "if 'moments_hu' in scan_parameters['prop_for_comparison']:\n",
    "    scan_parameters['compare_weight'] = [1 for i in range(6 + len(scan_parameters['prop_for_comparison']))]\n",
    "else:\n",
    "    scan_parameters['compare_weight'] = [1 for i in range(len(scan_parameters['prop_for_comparison']))]\n",
    "scan_parameters['compare_weight'] = np.array(scan_parameters['compare_weight'])/ float(np.sum(scan_parameters['compare_weight']))\n",
    "scan_parameters['similarity_threshold'] = 0.8\n",
    "scan_parameters['o_fix_scan_size'] = True\n",
    "scan_parameters['scan_size'] = 112\n",
    "scan_parameters['scan_size_coeff'] = 5\n",
    "scan_parameters['builtInProps'] = ['centroid','orientation', 'eccentricity','area','orientation','moments_hu','bbox','equivalent_diameter','label','local_centroid','major_axis_length','solidity','minor_axis_length','perimeter','solidity']\n",
    "scan_parameters['prop_to_save'] = ['coords','moments_hu','centroid','area','eccentricity','equivalent_diameter']\n",
    "scan_parameters['scan_outside_margin'] = 100\n",
    "stack = scan_parameters['stack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive \"s3://mousebrainatlas-data/CSHL_labelings_v3/MD589\" \"/shared/CSHL_labelings_v3/MD589\" --exclude \"*\" --include \"*contours*\"\n",
      "latest timestamp:  07292017045157\n"
     ]
    }
   ],
   "source": [
    "contour_df = DataManager.load_annotation_v4(stack=scan_parameters['stack'],by_human=True,suffix='contours',timestamp='latest')\n",
    "contour_df = convert_annotation_v3_original_to_aligned_cropped(contour_df=contour_df,stack=stack)\n",
    "structure_name = '7N'\n",
    "vertice_7N_dic = {record['section']:np.array(record['vertices'],np.int) for _, record in contour_df[contour_df['name']==structure_name].iterrows()}\n",
    "sectionList_7N = np.sort(vertice_7N_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "margin = 500\n",
    "bboxs_7N = {sec:fun_polygon_bbox(vertice_7N_dic[sec]) for sec in sectionList_7N}\n",
    "bbox_7N = fun_polygons_bbox(bboxs_7N.values(),margin=margin)\n",
    "# bbox_7N = bboxs_7N[154]\n",
    "scan_parameters['crop_range_mmxx'] = bbox_7N\n",
    "scan_parameters['crop_0_min'], scan_parameters['crop_1_min'],scan_parameters['crop_0_max'],scan_parameters['crop_1_max'] = bbox_7N \n",
    "scan_parameters['im0max'] = scan_parameters['crop_0_max'] - scan_parameters['crop_0_min']\n",
    "scan_parameters['im1max'] = scan_parameters['crop_1_max'] - scan_parameters['crop_1_min']\n",
    "scan_parameters['crop_range_mxmx'] = fun_mmxx_to_mxmx(*scan_parameters['crop_range_mmxx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:2438: DecompressionBombWarning: Image size (185246720 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "image = {}\n",
    "stack = scan_parameters['stack']\n",
    "section_begin, section_end = scan_parameters['section_limits']\n",
    "# secList = range(section_begin, section_end+1)\n",
    "# tempSecList = range(section_begin, section_end+1)\n",
    "secList = list(sectionList_7N[0:20])\n",
    "tempSecList = list(sectionList_7N[0:20])\n",
    "for tempSec in tempSecList:\n",
    "    try:\n",
    "        image[tempSec] = fun_crop_images(DataManager.load_image_v2(stack=stack, section=tempSec, version='jpeg', prep_id=2),*scan_parameters['crop_range_mmxx'],im0max=scan_parameters['oriImL0'],im1max=scan_parameters['oriImL1'] )\n",
    "    except:\n",
    "        sys.stderr.write('Invalid section %d...\\n'%tempSec)\n",
    "        secList.remove(tempSec)\n",
    "        continue\n",
    "#     if os.path.isfile(img_filename[tempSec]):\n",
    "#         sys.stderr.write('Image file is already available. \\n')\n",
    "#         continue\n",
    "#     else:\n",
    "#         try:\n",
    "#             download_from_s3(img_filename[tempSec])\n",
    "#         except:\n",
    "#             sys.stderr.write('Download fail. Skip this file for section %d...\\n'%tempSec)\n",
    "#             secList.remove(tempSec)\n",
    "#             continue   \n",
    "scan_parameters['secList'] = secList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: missing section 151"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 152 Finished percentage: 0.000000\n",
      "Section 152 Finished percentage: 42.863266\n",
      "Section 152 Finished percentage: 85.726532\n",
      "Section 153 Finished percentage: 0.000000\n",
      "Section 153 Finished percentage: 57.372347\n",
      "Section 154 Finished percentage: 0.000000\n",
      "Section 154 Finished percentage: 45.977011\n",
      "Section 154 Finished percentage: 91.954023\n",
      "Section 155 Finished percentage: 0.000000\n",
      "Section 155 Finished percentage: 37.965072\n",
      "Section 155 Finished percentage: 75.930144\n",
      "Section 156 Finished percentage: 0.000000\n",
      "Section 156 Finished percentage: 59.630292\n",
      "Section 157 Finished percentage: 0.000000\n",
      "Section 157 Finished percentage: 40.241449\n",
      "Section 157 Finished percentage: 80.482897\n",
      "Section 158 Finished percentage: 0.000000\n",
      "Section 158 Finished percentage: 55.586437\n",
      "Section 159 Finished percentage: 0.000000\n",
      "Section 159 Finished percentage: 38.744673\n",
      "Section 159 Finished percentage: 77.489345\n",
      "Section 160 Finished percentage: 0.000000\n",
      "Section 160 Finished percentage: 36.523009\n",
      "Section 160 Finished percentage: 73.046019\n",
      "Section 161 Finished percentage: 0.000000\n",
      "Section 161 Finished percentage: 31.113877\n",
      "Section 161 Finished percentage: 62.227754\n",
      "Section 161 Finished percentage: 93.341630\n",
      "Section 162 Finished percentage: 0.000000\n",
      "Section 162 Finished percentage: 36.496350\n",
      "Section 162 Finished percentage: 72.992701\n",
      "Section 163 Finished percentage: 0.000000\n",
      "Section 163 Finished percentage: 30.165913\n",
      "Section 163 Finished percentage: 60.331825\n",
      "Section 163 Finished percentage: 90.497738\n",
      "Section 164 Finished percentage: 0.000000\n",
      "Section 164 Finished percentage: 33.692722\n",
      "Section 164 Finished percentage: 67.385445\n",
      "Section 165 Finished percentage: 0.000000\n",
      "Section 165 Finished percentage: 27.700831\n",
      "Section 165 Finished percentage: 55.401662\n",
      "Section 165 Finished percentage: 83.102493\n",
      "Section 166 Finished percentage: 0.000000\n",
      "Section 166 Finished percentage: 28.145229\n",
      "Section 166 Finished percentage: 56.290459\n",
      "Section 166 Finished percentage: 84.435688\n",
      "Section 167 Finished percentage: 0.000000\n",
      "Section 167 Finished percentage: 34.891835\n",
      "Section 167 Finished percentage: 69.783671\n",
      "Section 168 Finished percentage: 0.000000\n",
      "Section 168 Finished percentage: 33.422460\n",
      "Section 168 Finished percentage: 66.844920\n",
      "Section 169 Finished percentage: 0.000000\n",
      "Section 169 Finished percentage: 28.392959\n",
      "Section 169 Finished percentage: 56.785917\n",
      "Section 169 Finished percentage: 85.178876\n",
      "Section 170 Finished percentage: 0.000000\n",
      "Section 170 Finished percentage: 31.847134\n",
      "Section 170 Finished percentage: 63.694268\n",
      "Section 170 Finished percentage: 95.541401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: missing section 172"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 171 Finished percentage: 0.000000\n",
      "Section 171 Finished percentage: 26.645350\n",
      "Section 171 Finished percentage: 53.290701\n",
      "Section 171 Finished percentage: 79.936051\n"
     ]
    }
   ],
   "source": [
    "scan_range = scan_parameters['scan_section_range']\n",
    "im0max = scan_parameters['im0max']\n",
    "im1max = scan_parameters['im1max']\n",
    "prop = scan_parameters['prop']\n",
    "prop_for_comparison = scan_parameters['prop_for_comparison']\n",
    "compare_weight = scan_parameters['compare_weight']\n",
    "o_simil_threshold = scan_parameters['similarity_threshold']\n",
    "o_fix_scan_size = scan_parameters['o_fix_scan_size']\n",
    "o_scan_size_coeff = scan_parameters['scan_size_coeff']\n",
    "o_scan_size = scan_parameters['scan_size']  \n",
    "secList = scan_parameters['secList'] \n",
    "stack = scan_parameters['stack']\n",
    "\n",
    "data_typical_blobs = {}\n",
    "data_matched_paris = {}\n",
    "data_false_typical_blobs = {}\n",
    "data_false_matched_pairs = {}\n",
    "data_blob_prop_dic = {}\n",
    "data_blob_idx_selection = {}\n",
    "# section = 154\n",
    "for section in secList:\n",
    "    typical_blobs = []\n",
    "    matched_paris = []\n",
    "    false_typical_blobs = []\n",
    "    false_matched_paris = []\n",
    "    cell_centroids = {}\n",
    "    cell_numbers = {}\n",
    "    cell_global_coord = {}\n",
    "    im_blob_prop = {}\n",
    "    im_label_ori = {}\n",
    "    im_label = {}\n",
    "    im_BW = {}\n",
    "    sec_load_data_list = range(section - scan_range, section + scan_range + 1)\n",
    "    scan_section = list(sec_load_data_list)\n",
    "    scan_section.remove(section)\n",
    "### loading data and reconstructed labeled images ###\n",
    "    tempList = list(sec_load_data_list)\n",
    "    for tempSec in tempList:\n",
    "#         print('process section %d'%tempSec)\n",
    "        if tempSec in secList:\n",
    "            cell_global_coord[tempSec] = load_cell_data('coords', stack=stack, sec=tempSec)\n",
    "#             print('load section %d'%tempSec)\n",
    "            temp_im_label, temp_im_blob_prop, _ = fun_reconstruct_labeled_image(cell_global_coord[tempSec],crop_range= scan_parameters['crop_range_mxmx'], \n",
    "                                                                        oriImL0=scan_parameters['oriImL0'],oriImL1=scan_parameters['oriImL1'])\n",
    "            im_label[tempSec] = temp_im_label\n",
    "            im_BW[tempSec] = temp_im_label > 0\n",
    "            im_blob_prop[tempSec] = np.array(temp_im_blob_prop)\n",
    "        else:\n",
    "            sys.stderr.write('Warning: missing section %d'%tempSec)\n",
    "#             print(scan_section)\n",
    "            scan_section.remove(tempSec)\n",
    "#             print(scan_section)\n",
    "#             print(sec_load_data_list)\n",
    "            sec_load_data_list.remove(tempSec)\n",
    "#             print(sec_load_data_list)\n",
    "            \n",
    "### Start getting region properties of each blob    ###\n",
    "    blob_prop_dic = fun_regionprops_dic(im_blob_prop=im_blob_prop,scan_parameters=scan_parameters)\n",
    "    data_blob_prop_dic[section] = blob_prop_dic[section] # Save data\n",
    "    # Eliminating too large and too small blobs\n",
    "    blob_idx_selection = {tempSec : np.logical_and.reduce(np.row_stack(\n",
    "        (blob_prop_dic[tempSec]['compactness']>0,\n",
    "    #      blob_prop_dic[tempSec]['compactness']<2.5,\n",
    "         blob_prop_dic[tempSec]['area']<3000,\n",
    "         blob_prop_dic[tempSec]['area']>200))) for tempSec in sec_load_data_list}  \n",
    "    \n",
    "    data_blob_idx_selection[section] = blob_idx_selection[section] # Save data\n",
    "    \n",
    "    n_blobs = {tempSec: len(im_blob_prop[tempSec]) for tempSec in im_blob_prop.keys()}\n",
    "    secList_in_BlobPropDic = im_blob_prop.keys()\n",
    "    if set(scan_section).issubset(set(secList_in_BlobPropDic)):\n",
    "        pass\n",
    "    else:\n",
    "        print('Warrning: Scaned section(s) not included in input im_blob_prop')\n",
    "\n",
    "        \n",
    "        \n",
    "### Start scanning ###\n",
    "    for blobID in range(n_blobs[section]):\n",
    "        if (blobID % 1000 == 0):\n",
    "            print('Section %d Finished percentage: %f'%(section, (float(blobID)*100 / n_blobs[section]) ))\n",
    "        \n",
    "        if not blob_idx_selection[section][blobID]:\n",
    "#             print('Blob %d eliminated'%blobID)  # If not the selected blob, skip the following steps\n",
    "            continue\n",
    "        \n",
    "        temp_curr_blob_props = {}\n",
    "        for tempProp in prop:\n",
    "            temp_curr_blob_props[tempProp] = blob_prop_dic[section][tempProp][blobID]\n",
    "        # Get the scan region bouding box\n",
    "        tempB1_idx_loc = temp_curr_blob_props['centroid']\n",
    "        if o_fix_scan_size:\n",
    "            temp_next_sec_range, local_cloc = fun_scan_range(tempB1_idx_loc,o_scan_size,im0max=im0max,im1max=im1max,o_form='2D')\n",
    "            temp_next_sec_range_1D,_ = fun_scan_range(tempB1_idx_loc,o_scan_size,im0max=im0max,im1max=im1max)\n",
    "        else:\n",
    "            temp_next_sec_range, local_cloc = fun_scan_range(tempB1_idx_loc,o_scan_size_coeff*fun_radius_bbox(*temp_curr_blob_props.bbox),im0max=im0max,im1max=im1max,o_form='2D')\n",
    "            temp_next_sec_range_1D,_ = fun_scan_range(tempB1_idx_loc,o_scan_size_coeff*fun_radius_bbox(*temp_curr_blob_props.bbox),im0max=im0max,im1max=im1max)\n",
    "\n",
    "\n",
    "        for tempSec in scan_section:\n",
    "    #         print('Blbo %d left. Start scanning'%blobID)\n",
    "            if tempSec not in secList_in_BlobPropDic:\n",
    "                continue\n",
    "\n",
    "            # Find blobs at the nearby location in the scaned section\n",
    "            # Method 1\n",
    "            tempBlobInside = fun_blobs_in_polygen(blob_prop_dic[tempSec]['centroid'],temp_next_sec_range,coor_order='cr')\n",
    "#             tempBlobOutside = fun_blobs_out_polygen(blob_prop_dic[tempSec]['centroid'], temp_next_sec_range, coor_order='cr', margin=scan_parameters['scan_outside_margin'])\n",
    "                # Use the annotated boundary\n",
    "            tempBlobOutside = fun_blobs_out_polygen(blob_prop_dic[tempSec]['centroid'], vertice_7N_dic[tempSec], coor_order='rc', margin=scan_parameters['scan_outside_margin'])\n",
    "    #         tempPath = matplotlib.path.Path(temp_next_sec_range)\n",
    "    #         tempBlobInside = tempPath.contains_points(blob_prop_dic[tempSec]['centroid'])\n",
    "            tempBlobInsideIndex = np.where(tempBlobInside)[0]\n",
    "            temp_num_blob = len(tempBlobInsideIndex)\n",
    "            tempBlobOutsideIndex = np.where(tempBlobOutside)[0]\n",
    "            temp_num_outblob = len(tempBlobOutsideIndex)\n",
    "\n",
    "            if temp_num_blob:\n",
    "                temp_sim = {}\n",
    "                for temp_prop in prop_for_comparison:\n",
    "                          temp_sim[temp_prop] = np.array(fun_similarity(temp_curr_blob_props[temp_prop], blob_prop_dic[tempSec][temp_prop][tempBlobInside],distance_type=temp_prop))\n",
    "                temp_sim_matrix = np.column_stack((temp_sim[temp_prop] for temp_prop in prop_for_comparison))\n",
    "\n",
    "                #### Blob comparison ####\n",
    "                temp_weighted_sim = np.dot(temp_sim_matrix,compare_weight)\n",
    "                temp_compare_result = temp_weighted_sim > o_simil_threshold\n",
    "                if any(temp_compare_result.tolist()):\n",
    "    #                 print('Typical blob found: Blob %d similarity %f'%(blobID,max(temp_weighted_sim)))\n",
    "                    typical_blobs.append([section,blobID, im_blob_prop[section][blobID]])\n",
    "                    matched_paris.append([section,blobID,\n",
    "                                          im_blob_prop[section][blobID],\n",
    "                                          tempSec, \n",
    "                                          tempBlobInsideIndex[temp_compare_result], \n",
    "                                          im_blob_prop[tempSec][tempBlobInsideIndex[temp_compare_result]],\n",
    "                                          temp_sim_matrix[temp_compare_result,:]])\n",
    "    #             temp_next_sec_blob_prop = np.array(im_blob_prop[tempSec])[tempBlobInsideIndex]\n",
    "            else:\n",
    "    #             print('No blobs found in this section')\n",
    "                pass\n",
    "\n",
    "\n",
    "            if temp_num_outblob:\n",
    "                temp_false_sim = {}\n",
    "                for temp_prop in prop_for_comparison:\n",
    "                    temp_false_sim[temp_prop] = np.array(fun_similarity(temp_curr_blob_props[temp_prop], blob_prop_dic[tempSec][temp_prop][tempBlobOutside],distance_type=temp_prop))\n",
    "                temp_false_sim_matrix = np.column_stack((temp_false_sim[temp_prop] for temp_prop in prop_for_comparison))\n",
    "\n",
    "                #### Blob comparison ####\n",
    "                temp_weighted_false_sim = np.dot(temp_false_sim_matrix,compare_weight)\n",
    "                temp_false_compare_result = temp_weighted_false_sim > o_simil_threshold\n",
    "                if any(temp_false_compare_result.tolist()):\n",
    "    #                 print('Typical blob found: Blob %d similarity %f'%(blobID,max(temp_weighted_sim)))\n",
    "                    false_typical_blobs.append([section,blobID, im_blob_prop[section][blobID]])\n",
    "                    false_matched_paris.append([section,blobID,im_blob_prop[section][blobID],\n",
    "                                                tempSec, \n",
    "                                                tempBlobOutsideIndex[temp_false_compare_result], \n",
    "                                                im_blob_prop[tempSec][tempBlobOutsideIndex[temp_false_compare_result]],\n",
    "                                                temp_false_sim_matrix[temp_false_compare_result,:]])\n",
    "    #             temp_next_sec_blob_prop = np.array(im_blob_prop[tempSec])[tempBlobInsideIndex]\n",
    "            else:\n",
    "    #             print('No blobs found in this section')\n",
    "                pass\n",
    "    data_typical_blobs[section] = typical_blobs\n",
    "    data_matched_paris[section] = matched_paris\n",
    "    data_false_typical_blobs[section] = false_typical_blobs\n",
    "    data_false_matched_pairs[section] = false_matched_paris   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fun_get_regionprops(skimage_measure_region, prop):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        skimage_measure_region: regionprops, or a list/ tuple of regionprops. regionprops is returned by skimage.measure.regionprps(), of the type skimage.measure.regionprops\n",
    "        prop: region property, can be built-in properties like 'area', 'moments_hu', eccentricity', or in addition, 'compactness'\n",
    "    Returns: (a list/tuple of) regionprops[prop] in float/numpy.ndarray/int, depends on the input.\n",
    "    \n",
    "    \"\"\"\n",
    "    buildinProps = ['centroid','orientation', 'eccentricity','area','orientation','moments_hu','bbox','equivalent_diameter','label','local_centroid','major_axis_length','solidity','minor_axis_length','perimeter','solidity']\n",
    "    PI = 3.1415926\n",
    "    if type(skimage_measure_region) in [list, tuple]:\n",
    "#         print(type(skimage_measure_region))\n",
    "        num_region = len(skimage_measure_region)\n",
    "        if prop in buildinProps:\n",
    "            return [skimage_measure_region[blobID][prop] for blobID in range(num_region)]\n",
    "        elif prop == 'compactness':\n",
    "            return [float(skimage_measure_region[blobID]['perimeter'])**2 / float(skimage_measure_region[blobID]['area']) / (4 * PI) for blobID in range(num_region)]\n",
    "    else:\n",
    "        if prop in buildinProps:\n",
    "            return skimage_measure_region[prop]\n",
    "        elif prop == 'compactness':\n",
    "            return float(skimage_measure_region['perimeter'])**2 / float(skimage_measure_region['area']) / (4 * PI)\n",
    "\n",
    "def fun_matched_pairs_to_regionprop_pairs(record):\n",
    "    regionprop_pairs = []\n",
    "    for tempRP in record[5]:\n",
    "        regionprop_pairs.append([record[2], tempRP])\n",
    "    return regionprop_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skimage.measure._regionprops._RegionProperties"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_typical_blobs[152][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_typical_blobs_properties = {}\n",
    "for tempSec in data_typical_blobs.keys():\n",
    "    data_typical_blobs_properties[tempSec] = [record[2] for record in data_typical_blobs[tempSec]]\n",
    "\n",
    "data_typical_blobs_properties = fun_regionprops_dic(data_typical_blobs_properties, scan_parameters)\n",
    "data_typical_blobs_properties_flatten = {}\n",
    "# typical_blob_properties, might be dubplicated due to multiple sections matching\n",
    "for tempProp in scan_parameters['prop']:\n",
    "    data_typical_blobs_properties_flatten[tempProp] = np.concatenate(tuple([data_typical_blobs_properties[tempSec][tempProp] for tempSec in data_typical_blobs_properties.keys()]))\n",
    "# dd for delete-duplicated\n",
    "data_typical_blobs_properties_dd = {}\n",
    "for tempProp in scan_parameters['prop']:\n",
    "    data_typical_blobs_properties_dd[tempProp] =  np.concatenate(tuple([data_blob_prop_dic[tempSec][tempProp][np.unique(data_typical_blobs_properties[tempSec]['label'])-1] for tempSec in data_blob_prop_dic.keys()]))\n",
    "# All detected blob properties in the cropped region\n",
    "data_blob_prop_dic_flatten = {}\n",
    "data_selected_blob_props_flatten_dic = {}\n",
    "for tempProp in scan_parameters['prop']:\n",
    "    data_blob_prop_dic_flatten[tempProp] = np.concatenate(tuple([data_blob_prop_dic[tempSec][tempProp] for tempSec in data_blob_prop_dic.keys()]))\n",
    "# All the selected blob properties in the cropped region\n",
    "temp_selected_idx_flatten = np.logical_and(data_blob_prop_dic_flatten['area']>200, data_blob_prop_dic_flatten['area']<3000)\n",
    "for tempProp in scan_parameters['prop']:\n",
    "    data_selected_blob_props_flatten_dic[tempProp] = data_blob_prop_dic_flatten[tempProp][temp_selected_idx_flatten]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempStructure = '7N'\n",
    "typical_blob_idx_7N = {tempSec : np.where(fun_blobs_in_polygen(data_typical_blobs_properties[tempSec]['centroid'], \n",
    "                                                  vertice_7N_dic[tempSec], \n",
    "                                                  crop_min_list=[scan_parameters['crop_0_min'], scan_parameters['crop_1_min']]))[0] \n",
    "                   for tempSec in data_typical_blobs_properties.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the regionprops for matched pairs of 7n (positive examples)\n",
    "data_matched_regionprop_pairs = {}\n",
    "for tempSec in data_matched_paris.keys():\n",
    "    data_matched_regionprop_pairs[tempSec] = []\n",
    "    for record in np.array(data_matched_paris[tempSec])[typical_blob_idx_7N[tempSec]]: # Select pairs that are inside 7N\n",
    "        data_matched_regionprop_pairs[tempSec] = data_matched_regionprop_pairs[tempSec] + fun_matched_pairs_to_regionprop_pairs(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scan_parameters['props_for_training'] = ['area', 'eccentricity', 'orientation', 'compactness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_similarity_vec_dic = {}\n",
    "for tempSec in data_matched_regionprop_pairs.keys():\n",
    "    positive_similarity_vec_dic[tempSec] = {}\n",
    "    for tempProp in scan_parameters['props_for_training']:\n",
    "        positive_similarity_vec_dic[tempSec][tempProp] = []\n",
    "        for record in data_matched_regionprop_pairs[tempSec]:\n",
    "            positive_similarity_vec_dic[tempSec][tempProp].append( fun_similarity(*fun_get_regionprops(record, tempProp), distance_type=tempProp))\n",
    "# Get similarity matrix of positive examples for each section\n",
    "positive_similarity_matrix_dic = {}\n",
    "for tempSec in positive_similarity_vec_dic.keys():\n",
    "    temp = []\n",
    "    for tempProp in scan_parameters['props_for_training']: # The orders matter\n",
    "        temp.append(np.array(positive_similarity_vec_dic[tempSec][tempProp]))\n",
    "        positive_similarity_matrix_dic[tempSec] = np.column_stack(tuple(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the blobs in 7N has mismatched with blobs outside\n",
    "data_false_typical_blobs_properties = {}\n",
    "for tempSec in data_false_typical_blobs.keys():\n",
    "    data_false_typical_blobs_properties[tempSec] = [record[2] for record in data_false_typical_blobs[tempSec]]\n",
    "data_false_typical_blobs_properties = fun_regionprops_dic(data_false_typical_blobs_properties, scan_parameters)\n",
    "\n",
    "flase_typical_blob_idx_7N = {tempSec : np.where(fun_blobs_in_polygen(data_false_typical_blobs_properties[tempSec]['centroid'], \n",
    "                                                  vertice_7N_dic[tempSec], \n",
    "                                                  crop_min_list=[scan_parameters['crop_0_min'], scan_parameters['crop_1_min']]))[0] \n",
    "                   for tempSec in data_false_typical_blobs_properties.keys()}\n",
    "\n",
    "data_false_matched_regionprop_pairs = {}\n",
    "for tempSec in data_false_matched_pairs.keys():\n",
    "    data_false_matched_regionprop_pairs[tempSec] = []\n",
    "    for record in np.array(data_false_matched_pairs[tempSec])[flase_typical_blob_idx_7N[tempSec]]:\n",
    "        data_false_matched_regionprop_pairs[tempSec] = data_false_matched_regionprop_pairs[tempSec] + fun_matched_pairs_to_regionprop_pairs(record) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_similarity_vec_dic = {}\n",
    "for tempSec in data_false_matched_regionprop_pairs.keys():\n",
    "#     print(tempSec)\n",
    "    negative_similarity_vec_dic[tempSec] = {}\n",
    "    for tempProp in scan_parameters['props_for_training']:\n",
    "        negative_similarity_vec_dic[tempSec][tempProp] = []\n",
    "        for record in data_false_matched_regionprop_pairs[tempSec]:\n",
    "            negative_similarity_vec_dic[tempSec][tempProp].append( fun_similarity(*fun_get_regionprops(record, tempProp), distance_type=tempProp) )\n",
    "# Takes about 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get similarity matrix of negative examples for each section\n",
    "negative_similarity_matrix_dic = {}\n",
    "for tempSec in negative_similarity_vec_dic.keys():\n",
    "    temp = []\n",
    "    for tempProp in scan_parameters['props_for_training']: # The orders matter -> cannot use .keys\n",
    "        temp.append(np.array(negative_similarity_vec_dic[tempSec][tempProp]))\n",
    "        negative_similarity_matrix_dic[tempSec] = np.column_stack(tuple(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "pool = Pool(NUM_CORES/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <type 'function'>: attribute lookup __builtin__.function failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-08e6ea8f5dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfun_get_regionprops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempProp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtempProp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_false_matched_regionprop_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtempSec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    249\u001b[0m         '''\n\u001b[1;32m    250\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mRUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <type 'function'>: attribute lookup __builtin__.function failed"
     ]
    }
   ],
   "source": [
    "pool.map(lambda record: fun_similarity(*fun_get_regionprops(record, tempProp), distance_type=tempProp), data_false_matched_regionprop_pairs[tempSec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_similarity_matrix = np.vstack(tuple([temp for temp in positive_similarity_matrix_dic.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_similarity_matrix = np.vstack(tuple([temp for temp in negative_similarity_matrix_dic.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_neg_trainingset = 5000\n",
    "size_pos_trainingset = 5000\n",
    "temp_neg_sample_idx = np.random.choice(range(len(negative_similarity_matrix)),size=(size_neg_trainingset,), replace=False)\n",
    "temp_pos_sample_idx = np.random.choice(range(len(positive_similarity_matrix)),size=(size_pos_trainingset,), replace=False)\n",
    "negative_training_set = negative_similarity_matrix[temp_neg_sample_idx, :]\n",
    "positive_training_set = positive_similarity_matrix[temp_pos_sample_idx, :]\n",
    "training_set = np.vstack(tuple([positive_training_set, negative_training_set]))\n",
    "training_label = np.concatenate(tuple([ np.ones(size_pos_trainingset,dtype=np.bool), np.zeros(size_neg_trainingset, dtype=np.bool)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.500000\n",
      "Coefficients are: [[ 1.82926773  1.15662045  2.36857025 -0.22471076]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:73: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model_perceptron = linear_model.perceptron.Perceptron(fit_intercept=True,n_iter=10,shuffle=True).fit(training_set,training_label)\n",
    "training_accuracy = model_perceptron.score(training_set,training_label)\n",
    "print 'The training accuracy is %f' %training_accuracy\n",
    "print 'Coefficients are:', model_perceptron.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
