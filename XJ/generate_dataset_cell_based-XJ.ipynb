{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD658/MD658_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD658/MD658_cropbox.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'cells'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "from cell_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network_model</th>\n",
       "      <th>stain</th>\n",
       "      <th>margins</th>\n",
       "      <th>num_sample_per_class</th>\n",
       "      <th>stacks</th>\n",
       "      <th>cell_features_used</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeSizeHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeSizeHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeOrientationHist/largeSizeHist/largeLargeL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeOrientationHist/largeSizeHist/largeLargeL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>allSizeHist/largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>allSizeHist/huMomentsHist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           network_model  stain  margins  num_sample_per_class stacks  \\\n",
       "dataset_id                                                              \n",
       "20          Inception-BN  nissl  200/500                  1000  MD585   \n",
       "21          Inception-BN  nissl  200/500                  1000  MD589   \n",
       "22          Inception-BN  nissl  200/500                  1000  MD594   \n",
       "92                   NaN  nissl      500                  1000  MD589   \n",
       "93                   NaN  nissl      500                  1000  MD594   \n",
       "94                   NaN  nissl      500                  1000  MD589   \n",
       "95                   NaN  nissl      500                  1000  MD594   \n",
       "96                   NaN  nissl      500                  1000  MD589   \n",
       "97                   NaN  nissl      500                  1000  MD594   \n",
       "98                   NaN  nissl      500                  1000  MD594   \n",
       "99                   NaN  nissl      500                  1000  MD589   \n",
       "100                  NaN  nissl      500                  1000  MD589   \n",
       "101                  NaN  nissl      500                  1000  MD589   \n",
       "\n",
       "                                           cell_features_used  \n",
       "dataset_id                                                     \n",
       "20                                                        NaN  \n",
       "21                                                        NaN  \n",
       "22                                                        NaN  \n",
       "92                                      largeLargeLinkLenHist  \n",
       "93                                      largeLargeLinkLenHist  \n",
       "94                                              largeSizeHist  \n",
       "95                                              largeSizeHist  \n",
       "96                        largeSizeHist/largeLargeLinkLenHist  \n",
       "97                        largeSizeHist/largeLargeLinkLenHist  \n",
       "98          largeOrientationHist/largeSizeHist/largeLargeL...  \n",
       "99          largeOrientationHist/largeSizeHist/largeLargeL...  \n",
       "100           allSizeHist/largeSizeHist/largeLargeLinkLenHist  \n",
       "101                                 allSizeHist/huMomentsHist  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'No object named structures in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotation has no structures.\n",
      "Load annotation. Time: 1.43 seconds.\n",
      "Regions on section 118 are not sampled because the section is invalid.\n",
      "Regions on section 221 are not sampled because the section is invalid.\n",
      "Regions on section 300 are not sampled because the section is invalid.\n",
      "Regions on section 324 are not sampled because the section is invalid.\n",
      "Sample addresses (stack MD589): 1. seconds.\n",
      "Aggregate addresses: 0.03 seconds\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: RuntimeWarning: invalid value encountered in divide\n",
      "Map addresses to features: 62.31 seconds\n",
      "/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py:1138: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->values] [items->None]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp /shared/CSHL_cells_v2/classifiers/datasets/dataset_101/patch_features.hdf s3://mousebrainatlas-data/CSHL_cells_v2/classifiers/datasets/dataset_101/patch_features.hdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.74 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp /shared/CSHL_cells_v2/classifiers/datasets/dataset_101/patch_addresses.pkl s3://mousebrainatlas-data/CSHL_cells_v2/classifiers/datasets/dataset_101/patch_addresses.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.42 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset_id = 101\n",
    "\n",
    "# Get information from the dataset[id]\n",
    "dataset_properties = dataset_settings.loc[dataset_id]\n",
    "num_samples_per_label = dataset_properties['num_sample_per_class']\n",
    "stacks = dataset_properties['stacks'].split('/')\n",
    "cell_features_used = dataset_properties['cell_features_used'].split('/')\n",
    "\n",
    "# Generate labels in the training set\n",
    "structures_to_sample = all_known_structures # Pick the labeled structure name for training\n",
    "# negative_labels_to_sample = [s + '_negative' for s in structures_to_sample]\n",
    "margins_to_sample = map(int, str(dataset_properties['margins']).split('/'))\n",
    "surround_positive_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix=surr_l) \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample \n",
    "                             for surr_l in structures_to_sample\n",
    "                             if surr_l != s]\n",
    "surround_noclass_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix='noclass') \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample]\n",
    "labels_to_sample = structures_to_sample + surround_positive_labels_to_sample + surround_noclass_labels_to_sample + ['bg']\n",
    "\n",
    "# Identify region indeices by section by label\n",
    "def identify_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        dict {label: list of region indices}\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        sys.stderr.write('Regions on section %d are not identified because the section is invalid.\\n' % sec)\n",
    "        return\n",
    "    \n",
    "    region_contours = load_cell_classifier_data(what='region_contours', stack=stack, sec=sec, ext='bp')\n",
    "    \n",
    "    surround_margins = set([get_margin_from_surround_label(label) for label in labels_to_sample if is_surround_label(label)])\n",
    "    region_labels = label_regions(stack=stack, section=sec, \n",
    "                                  region_contours=region_contours,\n",
    "                                  surround_margins=surround_margins,\n",
    "                                  labeled_contours=labeled_contours)\n",
    "    region_labels = {label: region_indices for label, region_indices in region_labels.iteritems()\n",
    "                     if len(region_indices) > 0}\n",
    "    return region_labels\n",
    "\n",
    "def sample_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours, num_samples_per_label=None):\n",
    "    \"\"\"\n",
    "    Sample region addresses in a particular section and associate them with different labels (positive, surround, negative, noclass, foreground, background, etc.).\n",
    "    \n",
    "    Args:\n",
    "        region_contours (list of nx2 arrays): list of contour vertices.\n",
    "        labeled_contours (dict of nx2 arrays): {label: contour vertices}\n",
    "        \n",
    "    Returns:\n",
    "        dict of 3-tuple list: {label: list of (stack, section, region_index)}.\n",
    "        If section is invalid, return None.\n",
    "    \"\"\"\n",
    "\n",
    "    addresses = {}\n",
    "    \n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        sys.stderr.write('Regions on section %d are not sampled because the section is invalid.\\n' % sec)\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        region_labels = load_hdf_v2(DataManager.get_region_labels_filepath(stack=stack, sec=sec))\n",
    "        region_labels = {label: region_indices for label, region_indices in region_labels.iteritems()\n",
    "                        if label in labels_to_sample}\n",
    "    except Exception as e:\n",
    "        sys.stderr.write('%s\\n' % e)\n",
    "        sys.stderr.write('Cannot load pre-computed region labels for section %d... re-compute\\n' % sec)\n",
    "        region_labels = identify_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours)\n",
    "\n",
    "    for label, region_indices in region_labels.iteritems():\n",
    "        if num_samples_per_label is None:\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in region_indices]\n",
    "        else:\n",
    "            sampled_region_indices = np.random.choice(region_indices, min(num_samples_per_label, len(region_indices)), replace=False)\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in sampled_region_indices]\n",
    "\n",
    "    return addresses\n",
    "\n",
    "def load_cell_based_features_one_section_(stack, section, region_indices, cell_features_used):\n",
    "    \"\"\"\n",
    "    Load pre-computed cell-based features for a list of regions on a particular section.\n",
    "    \"\"\"\n",
    "    \n",
    "    region_features_all_regions = load_cell_classifier_data(what='region_features', stack=stack, sec=section, ext='hdf')\n",
    "    # Loading hdf ~ 2 seconds.\n",
    "    \n",
    "    all_features_normalized = []\n",
    "    for feature_name in cell_features_used:\n",
    "        feats = np.asarray([region_features_all_regions[region_ind][feature_name] for region_ind in region_indices])\n",
    "        \n",
    "        # huMomentsHist feats: n_regions x 7 x n_bins\n",
    "        # other feats: n_regions x n_bins\n",
    "        \n",
    "        feats_normalized = feats/feats.sum(axis=-1)[...,None].astype(np.float)\n",
    "        if feature_name == 'huMomentsHist':\n",
    "            feats_normalized = feats_normalized.reshape((feats_normalized.shape[0], -1))\n",
    "        all_features_normalized.append(feats_normalized)\n",
    "\n",
    "    features = np.hstack(all_features_normalized)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def generate_dataset_cell_based(num_samples_per_label, stacks, labels_to_sample, cell_features_used):\n",
    "    \"\"\"\n",
    "    Generate dataset.\n",
    "    - Extract addresses\n",
    "    - Map addresses to features\n",
    "    - Remove None features\n",
    "    \n",
    "    Returns:\n",
    "        features, addresseslab\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample addresses\n",
    "    \n",
    "    addresses = defaultdict(list)\n",
    "    addresses_by_section_by_label = []\n",
    "    \n",
    "#     t = time.time()\n",
    "    \n",
    "    for stack in stacks:\n",
    "        \n",
    "        first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    \n",
    "        t1 = time.time()\n",
    "\n",
    "        contours_df, _ = DataManager.load_annotation_v3(stack=stack)\n",
    "        labeled_contours = contours_df[(contours_df['orientation'] == 'sagittal') & (contours_df['downsample'] == 1)].drop_duplicates(subset=['section', 'name', 'side', 'filename', 'downsample', 'creator'])\n",
    "        labeled_contours = convert_annotation_v3_original_to_aligned_cropped(labeled_contours, stack=stack)\n",
    "\n",
    "        sys.stderr.write('Load annotation. Time: %.2f seconds.\\n' % (time.time() - t1))\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # Sample addresses from each section\n",
    "\n",
    "        # If region labels are already computed, sequential loading is much faster (3s) than parallel loading (50s) due to disk read contention.\n",
    "        addresses_by_section_by_label_curr_stack = [\n",
    "            sample_predefined_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "                                                                 labels_to_sample=labels_to_sample,\n",
    "                                                                 labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "                                                                 num_samples_per_label=30)\n",
    "            for sec in range(first_sec, last_sec+1)]\n",
    "\n",
    "#         pool = Pool(NUM_CORES/2)\n",
    "#         addresses_by_section_by_label_curr_stack = \\\n",
    "#         pool.map(lambda sec: sample_predefined_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "#                                                                  labels_to_sample=labels_to_sample,\n",
    "#                                                                  labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "#                                                                  num_samples_per_label=30),\n",
    "#                  range(first_sec, last_sec+1))\n",
    "#         pool.close()\n",
    "#         pool.join()\n",
    "                \n",
    "        addresses_by_section_by_label += addresses_by_section_by_label_curr_stack\n",
    "\n",
    "        sys.stderr.write('Sample addresses (stack %s): %.2s seconds.\\n' % (stack, time.time() - t1))\n",
    "        \n",
    "    # Aggregate addresses sampled form each section\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    addresses_by_label = defaultdict(list)\n",
    "    for addrs_by_label in addresses_by_section_by_label:\n",
    "        if addrs_by_label is not None: # handle cases of invalid sections for which sampling function returns None\n",
    "            for label, addrs in addrs_by_label.iteritems():\n",
    "                addresses_by_label[label] += addrs\n",
    "    addresses_by_label.default_factory = None\n",
    "    \n",
    "    if num_samples_per_label is not None:\n",
    "        import random\n",
    "        addresses_by_label = {label: random.sample(addrs, min(num_samples_per_label/len(stacks), len(addrs))) \n",
    "                              for label, addrs in addresses_by_label.iteritems()}\n",
    "\n",
    "    # Remove unwanted labels\n",
    "    addresses_by_label = {label: addrs for label, addrs in addresses_by_label.iteritems() if label in labels_to_sample}\n",
    "    \n",
    "    sys.stderr.write('Aggregate addresses: %.2f seconds\\n' % (time.time() - t))    \n",
    "    \n",
    "    # Map addresses to features\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    load_cell_based_features_given_address_list = lambda addrs: smart_map(addrs, keyfunc=lambda (st, se, ri): (st, se),\\\n",
    "                        func=lambda (st, se), gr: \\\n",
    "                        load_cell_based_features_one_section_(stack=st, section=se, \n",
    "                                                              region_indices=[ri for _,_,ri in gr], cell_features_used=cell_features_used))\n",
    "    features_by_label = apply_function_to_dict(load_cell_based_features_given_address_list, addresses_by_label)\n",
    "    features_by_label = apply_function_to_dict(np.asarray, features_by_label)\n",
    "    \n",
    "    sys.stderr.write('Map addresses to features: %.2f seconds\\n' % (time.time() - t))\n",
    "    \n",
    "    # Remove features that are None are contain nan values.\n",
    "\n",
    "    for name in features_by_label.keys():\n",
    "        valid = [(ftr, addr) for ftr, addr in zip(features_by_label[name], addresses_by_label[name])\n",
    "                    if ftr is not None and not np.any(np.isnan(ftr))]\n",
    "        res = zip(*valid)\n",
    "        features_by_label[name] = np.array(res[0])\n",
    "        addresses_by_label[name] = res[1]\n",
    "    \n",
    "    return features_by_label, addresses_by_label\n",
    "\n",
    "# Generate training data set\n",
    "features, addresses = generate_dataset_cell_based(num_samples_per_label=num_samples_per_label, \n",
    "                                                  stacks=stacks,\n",
    "                                                  labels_to_sample=labels_to_sample,\n",
    "                                                 cell_features_used=cell_features_used)\n",
    "# Save training data set to '/shared/CSHL_cells_v2/classifiers/datasets/'\n",
    "features_fp = os.path.join(CELL_FEATURES_CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_features.hdf')\n",
    "create_parent_dir_if_not_exists(features_fp)\n",
    "save_hdf_v2(features, features_fp)\n",
    "upload_to_s3(features_fp)\n",
    "# train_feat_dir = create_if_not_exists(os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset, 'patch_features'))\n",
    "# for label, feats in training_features.iteritems():\n",
    "#     bp.pack_ndarray_file(feats, os.path.join(train_feat_dir, label + '.bp'))\n",
    "\n",
    "    # Save addresses\n",
    "addresses_fp = os.path.join(CELL_FEATURES_CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_addresses.pkl')\n",
    "save_pickle(addresses, addresses_fp)\n",
    "upload_to_s3(addresses_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features['10N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('MD589', 210, 36796),\n",
       " ('MD589', 229, 41148),\n",
       " ('MD589', 210, 36467),\n",
       " ('MD589', 211, 36857),\n",
       " ('MD589', 210, 37448),\n",
       " ('MD589', 211, 37524),\n",
       " ('MD589', 232, 39643),\n",
       " ('MD589', 232, 40260),\n",
       " ('MD589', 229, 41010),\n",
       " ('MD589', 210, 37121))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addresses['10N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10N 10\n",
      "10N_surround_500_12N 952\n",
      "10N_surround_500_noclass 998\n",
      "12N 1000\n",
      "12N_surround_500_10N 10\n",
      "12N_surround_500_AP 11\n",
      "12N_surround_500_noclass 969\n",
      "3N 689\n",
      "3N_surround_500_noclass 680\n",
      "4N_surround_500_3N 91\n",
      "4N_surround_500_noclass 335\n",
      "5N 1000\n",
      "5N_surround_500_7n 13\n",
      "5N_surround_500_noclass 992\n",
      "6N 23\n",
      "6N_surround_500_7n 8\n",
      "6N_surround_500_noclass 148\n",
      "7N 1000\n",
      "7N_surround_500_noclass 999\n",
      "7n 388\n",
      "7n_surround_500_5N 2\n",
      "7n_surround_500_6N 5\n",
      "7n_surround_500_noclass 960\n",
      "AP 333\n",
      "AP_surround_500_12N 18\n",
      "AP_surround_500_noclass 423\n",
      "Amb 205\n",
      "Amb_surround_500_7N 3\n",
      "Amb_surround_500_noclass 390\n",
      "DC 1000\n",
      "DC_surround_500_VCA 898\n",
      "DC_surround_500_VCP 994\n",
      "DC_surround_500_noclass 934\n",
      "IC 1000\n",
      "IC_surround_500_SC 1000\n",
      "IC_surround_500_noclass 921\n",
      "LC 713\n",
      "LC_surround_500_noclass 715\n",
      "LRt 1000\n",
      "LRt_surround_500_Sp5C 130\n",
      "LRt_surround_500_noclass 999\n",
      "PBG 259\n",
      "PBG_surround_500_SNR 17\n",
      "PBG_surround_500_VLL 98\n",
      "PBG_surround_500_noclass 777\n",
      "Pn 933\n",
      "Pn_surround_500_RtTg 178\n",
      "Pn_surround_500_VLL 24\n",
      "Pn_surround_500_noclass 898\n",
      "RMC 960\n",
      "RMC_surround_500_noclass 958\n",
      "RtTg 390\n",
      "RtTg_surround_500_Pn 224\n",
      "RtTg_surround_500_Tz 49\n",
      "RtTg_surround_500_noclass 394\n",
      "SC 1000\n",
      "SC_surround_500_IC 1000\n",
      "SC_surround_500_noclass 983\n",
      "SNC 571\n",
      "SNC_surround_500_SNR 987\n",
      "SNC_surround_500_noclass 987\n",
      "SNR 940\n",
      "SNR_surround_500_PBG 3\n",
      "SNR_surround_500_SNC 571\n",
      "SNR_surround_500_VLL 26\n",
      "SNR_surround_500_noclass 937\n",
      "Sp5C 998\n",
      "Sp5C_surround_500_LRt 18\n",
      "Sp5C_surround_500_Sp5I 630\n",
      "Sp5C_surround_500_noclass 992\n",
      "Sp5I 1000\n",
      "Sp5I_surround_500_Sp5C 630\n",
      "Sp5I_surround_500_Sp5O 1000\n",
      "Sp5I_surround_500_noclass 935\n",
      "Sp5O 987\n",
      "Sp5O_surround_500_Sp5I 999\n",
      "Sp5O_surround_500_noclass 889\n",
      "Tz 1000\n",
      "Tz_surround_500_RtTg 100\n",
      "Tz_surround_500_noclass 974\n",
      "VCA 1000\n",
      "VCA_surround_500_DC 878\n",
      "VCA_surround_500_VCP 1000\n",
      "VCA_surround_500_noclass 912\n",
      "VCP 991\n",
      "VCP_surround_500_DC 1000\n",
      "VCP_surround_500_VCA 972\n",
      "VCP_surround_500_noclass 902\n",
      "VLL 989\n",
      "VLL_surround_500_PBG 21\n",
      "VLL_surround_500_SNR 15\n",
      "VLL_surround_500_noclass 938\n",
      "bg 707\n"
     ]
    }
   ],
   "source": [
    "for l, v in sorted(addresses.items()):\n",
    "    print l, len(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
