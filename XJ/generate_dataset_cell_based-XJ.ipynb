{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'cells'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "from cell_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network_model</th>\n",
       "      <th>stain</th>\n",
       "      <th>margins</th>\n",
       "      <th>num_sample_per_class</th>\n",
       "      <th>stacks</th>\n",
       "      <th>cell_features_used</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>inception-bn-blue</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>inception-bn-blue</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>inception-bn-blue</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>inception-bn-blue</td>\n",
       "      <td>neurotrace_blue</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD642</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>allSizeHist/huMomentsHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeSizeHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeSizeHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeOrientationHist/largeSizeHist/largeLargeL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeOrientationHist/largeSizeHist/largeLargeL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>allSizeHist/largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>allSizeHist/huMomentsHist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                network_model            stain  margins  num_sample_per_class  \\\n",
       "dataset_id                                                                      \n",
       "20               Inception-BN            nissl  200/500                  1000   \n",
       "21               Inception-BN            nissl  200/500                  1000   \n",
       "22               Inception-BN            nissl  200/500                  1000   \n",
       "23          inception-bn-blue            nissl      500                  1000   \n",
       "24          inception-bn-blue            nissl      500                  1000   \n",
       "25          inception-bn-blue            nissl      500                  1000   \n",
       "50          inception-bn-blue  neurotrace_blue      500                  1000   \n",
       "90                        NaN            nissl      500                  1000   \n",
       "92                        NaN            nissl      500                  1000   \n",
       "93                        NaN            nissl      500                  1000   \n",
       "94                        NaN            nissl      500                  1000   \n",
       "95                        NaN            nissl      500                  1000   \n",
       "96                        NaN            nissl      500                  1000   \n",
       "97                        NaN            nissl      500                  1000   \n",
       "98                        NaN            nissl      500                  1000   \n",
       "99                        NaN            nissl      500                  1000   \n",
       "100                       NaN            nissl      500                  1000   \n",
       "101                       NaN            nissl      500                  1000   \n",
       "\n",
       "           stacks                                 cell_features_used  \n",
       "dataset_id                                                            \n",
       "20          MD585                                                NaN  \n",
       "21          MD589                                                NaN  \n",
       "22          MD594                                                NaN  \n",
       "23          MD585                                                NaN  \n",
       "24          MD589                                                NaN  \n",
       "25          MD594                                                NaN  \n",
       "50          MD642                                                NaN  \n",
       "90          MD589                          allSizeHist/huMomentsHist  \n",
       "92          MD589                              largeLargeLinkLenHist  \n",
       "93          MD594                              largeLargeLinkLenHist  \n",
       "94          MD589                                      largeSizeHist  \n",
       "95          MD594                                      largeSizeHist  \n",
       "96          MD589                largeSizeHist/largeLargeLinkLenHist  \n",
       "97          MD594                largeSizeHist/largeLargeLinkLenHist  \n",
       "98          MD594  largeOrientationHist/largeSizeHist/largeLargeL...  \n",
       "99          MD589  largeOrientationHist/largeSizeHist/largeLargeL...  \n",
       "100         MD589    allSizeHist/largeSizeHist/largeLargeLinkLenHist  \n",
       "101         MD589                          allSizeHist/huMomentsHist  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_id = 94\n",
    "\n",
    "# Get information from the dataset[id]\n",
    "dataset_properties = dataset_settings.loc[dataset_id]\n",
    "num_samples_per_label = dataset_properties['num_sample_per_class']\n",
    "stacks = dataset_properties['stacks'].split('/')\n",
    "cell_features_used = dataset_properties['cell_features_used'].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels in the training set\n",
    "structures_to_sample = all_known_structures # Pick the labeled structure name for training\n",
    "\n",
    "# negative_labels_to_sample = [s + '_negative' for s in structures_to_sample]\n",
    "margins_to_sample = map(int, str(dataset_properties['margins']).split('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify region indeices by section by label\n",
    "def identify_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        dict {label: list of region indices}\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        sys.stderr.write('Regions on section %d are not identified because the section is invalid.\\n' % sec)\n",
    "        return\n",
    "    \n",
    "    region_contours = load_cell_classifier_data(what='region_contours', stack=stack, sec=sec, ext='bp')\n",
    "    \n",
    "    surround_margins = set([get_margin_from_surround_label(label) for label in labels_to_sample if is_surround_label(label)])\n",
    "    region_labels = label_regions(stack=stack, section=sec, \n",
    "                                  region_contours=region_contours,\n",
    "                                  surround_margins=surround_margins,\n",
    "                                  labeled_contours=labeled_contours)\n",
    "    region_labels = {label: region_indices for label, region_indices in region_labels.iteritems()\n",
    "                     if len(region_indices) > 0}\n",
    "    return region_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours, num_samples_per_label=None):\n",
    "    \"\"\"\n",
    "    Sample region addresses in a particular section and associate them with different labels (positive, surround, negative, noclass, foreground, background, etc.).\n",
    "    \n",
    "    Args:\n",
    "        region_contours (list of nx2 arrays): list of contour vertices.\n",
    "        labeled_contours (dict of nx2 arrays): {label: contour vertices}\n",
    "        \n",
    "    Returns:\n",
    "        dict of 3-tuple list: {label: list of (stack, section, region_index)}.\n",
    "        If section is invalid, return None.\n",
    "    \"\"\"\n",
    "\n",
    "    addresses = {}\n",
    "    \n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        sys.stderr.write('Regions on section %d are not sampled because the section is invalid.\\n' % sec)\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        region_labels = load_hdf_v2(DataManager.get_region_labels_filepath(stack=stack, sec=sec))\n",
    "        region_labels = {label: region_indices for label, region_indices in region_labels.iteritems()\n",
    "                        if label in labels_to_sample}\n",
    "    except Exception as e:\n",
    "        sys.stderr.write('%s\\n' % e)\n",
    "        sys.stderr.write('Cannot load pre-computed region labels for section %d... re-compute\\n' % sec)\n",
    "        region_labels = identify_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours)\n",
    "\n",
    "    for label, region_indices in region_labels.iteritems():\n",
    "        if num_samples_per_label is None:\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in region_indices]\n",
    "        else:\n",
    "            sampled_region_indices = np.random.choice(region_indices, min(num_samples_per_label, len(region_indices)), replace=False)\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in sampled_region_indices]\n",
    "\n",
    "    return addresses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cell_based_features_one_section_(stack, section, region_indices, cell_features_used):\n",
    "    \"\"\"\n",
    "    Load pre-computed cell-based features for a list of regions on a particular section.\n",
    "    \"\"\"\n",
    "    \n",
    "    region_features_all_regions = load_cell_classifier_data(what='region_features', stack=stack, sec=section, ext='hdf')\n",
    "    # Loading hdf ~ 2 seconds.\n",
    "    \n",
    "    all_features_normalized = []\n",
    "    for feature_name in cell_features_used:\n",
    "        feats = np.asarray([region_features_all_regions[region_ind][feature_name] for region_ind in region_indices])\n",
    "        \n",
    "        # huMomentsHist feats: n_regions x 7 x n_bins\n",
    "        # other feats: n_regions x n_bins\n",
    "        \n",
    "        feats_normalized = feats/feats.sum(axis=-1)[...,None].astype(np.float)\n",
    "        if feature_name == 'huMomentsHist':\n",
    "            feats_normalized = feats_normalized.reshape((feats_normalized.shape[0], -1))\n",
    "        all_features_normalized.append(feats_normalized)\n",
    "\n",
    "    features = np.hstack(all_features_normalized)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_dataset_cell_based(num_samples_per_label, stacks, labels_to_sample, cell_features_used):\n",
    "    \"\"\"\n",
    "    Generate dataset.\n",
    "    - Extract addresses\n",
    "    - Map addresses to features\n",
    "    - Remove None features\n",
    "    \n",
    "    Returns:\n",
    "        features, addresseslab\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample addresses\n",
    "    \n",
    "    addresses = defaultdict(list)\n",
    "    addresses_by_section_by_label = []\n",
    "    \n",
    "#     t = time.time()\n",
    "    \n",
    "    for stack in stacks:\n",
    "        \n",
    "        first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    \n",
    "        t1 = time.time()\n",
    "\n",
    "        contours_df, _ = DataManager.load_annotation_v3(stack=stack)\n",
    "        labeled_contours = contours_df[(contours_df['orientation'] == 'sagittal') & (contours_df['downsample'] == 1)].drop_duplicates(subset=['section', 'name', 'side', 'filename', 'downsample', 'creator'])\n",
    "        labeled_contours = convert_annotation_v3_original_to_aligned_cropped(labeled_contours, stack=stack)\n",
    "\n",
    "        sys.stderr.write('Load annotation. Time: %.2f seconds.\\n' % (time.time() - t1))\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # Sample addresses from each section\n",
    "\n",
    "        # If region labels are already computed, sequential loading is much faster (3s) than parallel loading (50s) due to disk read contention.\n",
    "        addresses_by_section_by_label_curr_stack = [\n",
    "            sample_predefined_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "                                                                 labels_to_sample=labels_to_sample,\n",
    "                                                                 labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "                                                                 num_samples_per_label=30)\n",
    "            for sec in range(first_sec, last_sec+1)]\n",
    "\n",
    "#         pool = Pool(NUM_CORES/2)\n",
    "#         addresses_by_section_by_label_curr_stack = \\\n",
    "#         pool.map(lambda sec: sample_predefined_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "#                                                                  labels_to_sample=labels_to_sample,\n",
    "#                                                                  labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "#                                                                  num_samples_per_label=30),\n",
    "#                  range(first_sec, last_sec+1))\n",
    "#         pool.close()\n",
    "#         pool.join()\n",
    "                \n",
    "        addresses_by_section_by_label += addresses_by_section_by_label_curr_stack\n",
    "\n",
    "        sys.stderr.write('Sample addresses (stack %s): %.2s seconds.\\n' % (stack, time.time() - t1))\n",
    "        \n",
    "    # Aggregate addresses sampled form each section\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    addresses_by_label = defaultdict(list)\n",
    "    for addrs_by_label in addresses_by_section_by_label:\n",
    "        if addrs_by_label is not None: # handle cases of invalid sections for which sampling function returns None\n",
    "            for label, addrs in addrs_by_label.iteritems():\n",
    "                addresses_by_label[label] += addrs\n",
    "    addresses_by_label.default_factory = None\n",
    "    \n",
    "    if num_samples_per_label is not None:\n",
    "        import random\n",
    "        addresses_by_label = {label: random.sample(addrs, min(num_samples_per_label/len(stacks), len(addrs))) \n",
    "                              for label, addrs in addresses_by_label.iteritems()}\n",
    "\n",
    "    # Remove unwanted labels\n",
    "    addresses_by_label = {label: addrs for label, addrs in addresses_by_label.iteritems() if label in labels_to_sample}\n",
    "    \n",
    "    sys.stderr.write('Aggregate addresses: %.2f seconds\\n' % (time.time() - t))    \n",
    "    \n",
    "    # Map addresses to features\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    load_cell_based_features_given_address_list = lambda addrs: smart_map(addrs, keyfunc=lambda (st, se, ri): (st, se),\\\n",
    "                        func=lambda (st, se), gr: \\\n",
    "                        load_cell_based_features_one_section_(stack=st, section=se, \n",
    "                                                              region_indices=[ri for _,_,ri in gr], cell_features_used=cell_features_used))\n",
    "    features_by_label = apply_function_to_dict(load_cell_based_features_given_address_list, addresses_by_label)\n",
    "    features_by_label = apply_function_to_dict(np.asarray, features_by_label)\n",
    "    \n",
    "    sys.stderr.write('Map addresses to features: %.2f seconds\\n' % (time.time() - t))\n",
    "    \n",
    "    # Remove features that are None are contain nan values.\n",
    "\n",
    "    for name in features_by_label.keys():\n",
    "        valid = [(ftr, addr) for ftr, addr in zip(features_by_label[name], addresses_by_label[name])\n",
    "                    if ftr is not None and not np.any(np.isnan(ftr))]\n",
    "        res = zip(*valid)\n",
    "        features_by_label[name] = np.array(res[0])\n",
    "        addresses_by_label[name] = res[1]\n",
    "    \n",
    "    return features_by_label, addresses_by_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'No object named structures in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotation has no structures.\n",
      "Load annotation. Time: 1.32 seconds.\n",
      "File /shared/CSHL_cells_v2/classifiers/region_indices_by_label/MD589/MD589-N16-2015.07.30-17.03.43_MD589_3_0048_region_indices_by_label.hdf does not exist\n",
      "Cannot load pre-computed region labels for section 92... re-compute\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'get_margin_from_surround_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-616b018c1efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                   \u001b[0mstacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                   \u001b[0mlabels_to_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_to_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                                  cell_features_used=cell_features_used)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# Save training data set to '/shared/CSHL_cells_v2/classifiers/datasets/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeatures_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCELL_FEATURES_CLF_ROOTDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'patch_features.hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-31bf8fcaee2d>\u001b[0m in \u001b[0;36mgenerate_dataset_cell_based\u001b[0;34m(num_samples_per_label, stacks, labels_to_sample, cell_features_used)\u001b[0m\n\u001b[1;32m     39\u001b[0m                                                                  \u001b[0mlabeled_contours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabeled_contours\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabeled_contours\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                                                  num_samples_per_label=30)\n\u001b[0;32m---> 41\u001b[0;31m             for sec in range(first_sec, last_sec+1)]\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#         pool = Pool(NUM_CORES/2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-14b895af5d6c>\u001b[0m in \u001b[0;36msample_predefined_regions_one_section_by_label\u001b[0;34m(stack, sec, labels_to_sample, labeled_contours, num_samples_per_label)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot load pre-computed region labels for section %d... re-compute\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mregion_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentify_predefined_regions_one_section_by_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_contours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion_indices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregion_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-28be26d825d8>\u001b[0m in \u001b[0;36midentify_predefined_regions_one_section_by_label\u001b[0;34m(stack, sec, labels_to_sample, labeled_contours)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mregion_contours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cell_classifier_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'region_contours'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msurround_margins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_margin_from_surround_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_to_sample\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_surround_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     region_labels = label_regions(stack=stack, section=sec, \n\u001b[1;32m     16\u001b[0m                                   \u001b[0mregion_contours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregion_contours\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'get_margin_from_surround_label' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate training data set\n",
    "features, addresses = generate_dataset_cell_based(num_samples_per_label=num_samples_per_label, \n",
    "                                                  stacks=stacks,\n",
    "                                                  labels_to_sample=labels_to_sample,\n",
    "                                                 cell_features_used=cell_features_used)\n",
    "# Save training data set to '/shared/CSHL_cells_v2/classifiers/datasets/'\n",
    "features_fp = os.path.join(CELL_FEATURES_CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_features.hdf')\n",
    "create_parent_dir_if_not_exists(features_fp)\n",
    "save_hdf_v2(features, features_fp)\n",
    "upload_to_s3(features_fp)\n",
    "# train_feat_dir = create_if_not_exists(os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset, 'patch_features'))\n",
    "# for label, feats in training_features.iteritems():\n",
    "#     bp.pack_ndarray_file(feats, os.path.join(train_feat_dir, label + '.bp'))\n",
    "\n",
    "    # Save addresses\n",
    "addresses_fp = os.path.join(CELL_FEATURES_CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_addresses.pkl')\n",
    "save_pickle(addresses, addresses_fp)\n",
    "upload_to_s3(addresses_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features['10N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'addresses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-38f99cfa5015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maddresses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'10N'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'addresses' is not defined"
     ]
    }
   ],
   "source": [
    "addresses['10N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'addresses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a446271d4dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddresses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'addresses' is not defined"
     ]
    }
   ],
   "source": [
    "for l, v in sorted(addresses.items()):\n",
    "    print l, len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
