{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n",
      "rm -rf /shared/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt && mkdir -p /shared/CSHL_data_processed/MD635\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt /shared/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "Child returned 0\n",
      "Child returned 1\n",
      "0.39 seconds.\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt\n",
      "Child returned 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf /shared/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt && mkdir -p /shared/CSHL_data_processed/MD635\n",
      "aws s3 cp s3://mousebrainatlas-data/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt /shared/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 1\n",
      "0.36 seconds.\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_alignedTo_MD635-F63-2016.05.19-08.39.03_MD635_2_0188_cropbox.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'cells'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "from cell_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network_model</th>\n",
       "      <th>stain</th>\n",
       "      <th>margins</th>\n",
       "      <th>num_sample_per_class</th>\n",
       "      <th>stacks</th>\n",
       "      <th>cell_features_used</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>allSizeHist/huMomentsHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeSizeHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeSizeHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeOrientationHist/largeSizeHist/largeLargeL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeOrientationHist/largeSizeHist/largeLargeL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           network_model  stain  margins  num_sample_per_class stacks  \\\n",
       "dataset_id                                                              \n",
       "20          Inception-BN  nissl  200/500                  1000  MD585   \n",
       "21          Inception-BN  nissl  200/500                  1000  MD589   \n",
       "22          Inception-BN  nissl  200/500                  1000  MD594   \n",
       "90                   NaN  nissl      500                  1000  MD589   \n",
       "92                   NaN  nissl      500                  1000  MD589   \n",
       "93                   NaN  nissl      500                  1000  MD594   \n",
       "94                   NaN  nissl      500                  1000  MD589   \n",
       "95                   NaN  nissl      500                  1000  MD594   \n",
       "96                   NaN  nissl      500                  1000  MD589   \n",
       "97                   NaN  nissl      500                  1000  MD594   \n",
       "98                   NaN  nissl      500                  1000  MD594   \n",
       "99                   NaN  nissl      500                  1000  MD589   \n",
       "\n",
       "                                           cell_features_used  \n",
       "dataset_id                                                     \n",
       "20                                                        NaN  \n",
       "21                                                        NaN  \n",
       "22                                                        NaN  \n",
       "90                                  allSizeHist/huMomentsHist  \n",
       "92                                      largeLargeLinkLenHist  \n",
       "93                                      largeLargeLinkLenHist  \n",
       "94                                              largeSizeHist  \n",
       "95                                              largeSizeHist  \n",
       "96                        largeSizeHist/largeLargeLinkLenHist  \n",
       "97                        largeSizeHist/largeLargeLinkLenHist  \n",
       "98          largeOrientationHist/largeSizeHist/largeLargeL...  \n",
       "99          largeOrientationHist/largeSizeHist/largeLargeL...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File does not exist: /shared/CSHL_labelings_v3/MD589/MD589_annotation_v3.h5\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "File /shared/CSHL_labelings_v3/MD589/MD589_annotation_v3.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-215a97cc634c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m                                                   \u001b[0mstacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                                   \u001b[0mlabels_to_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_to_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                                  cell_features_used=cell_features_used)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;31m# Save training data set to '/shared/CSHL_cells_v2/classifiers/datasets/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0mfeatures_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCELL_FEATURES_CLF_ROOTDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset_%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'patch_features.hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-215a97cc634c>\u001b[0m in \u001b[0;36mgenerate_dataset_cell_based\u001b[0;34m(num_samples_per_label, stacks, labels_to_sample, cell_features_used)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mcontours_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_annotation_v3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mlabeled_contours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontours_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontours_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'orientation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sagittal'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontours_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'downsample'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'side'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'filename'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'downsample'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'creator'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mlabeled_contours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_annotation_v3_original_to_aligned_cropped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_contours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/MouseBrainAtlas/utilities/data_manager.pyc\u001b[0m in \u001b[0;36mload_annotation_v3\u001b[0;34m(stack, annotation_rootdir)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_annotation_v3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_rootdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mANNOTATION_ROOTDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_rootdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%(stack)s_annotation_v3.h5'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'stack'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mcontour_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiletype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'annotation_hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/MouseBrainAtlas/utilities/data_manager.pyc\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filepath, filetype)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'annotation_hdf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mcontour_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'contours'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontour_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfiletype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pickle'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             raise compat.FileNotFoundError(\n\u001b[0;32m--> 324\u001b[0;31m                 'File %s does not exist' % path_or_buf)\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# can't auto open/close if we are using an iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File /shared/CSHL_labelings_v3/MD589/MD589_annotation_v3.h5 does not exist"
     ]
    }
   ],
   "source": [
    "dataset_id = 94\n",
    "\n",
    "# Get information from the dataset[id]\n",
    "dataset_properties = dataset_settings.loc[dataset_id]\n",
    "num_samples_per_label = dataset_properties['num_sample_per_class']\n",
    "stacks = dataset_properties['stacks'].split('/')\n",
    "cell_features_used = dataset_properties['cell_features_used'].split('/')\n",
    "\n",
    "# Generate labels in the training set\n",
    "structures_to_sample = all_known_structures # Pick the labeled structure name for training\n",
    "# negative_labels_to_sample = [s + '_negative' for s in structures_to_sample]\n",
    "margins_to_sample = map(int, str(dataset_properties['margins']).split('/'))\n",
    "surround_positive_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix=surr_l) \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample \n",
    "                             for surr_l in structures_to_sample\n",
    "                             if surr_l != s]\n",
    "surround_noclass_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix='noclass') \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample]\n",
    "labels_to_sample = structures_to_sample + surround_positive_labels_to_sample + surround_noclass_labels_to_sample + ['bg']\n",
    "\n",
    "# Identify region indeices by section by label\n",
    "def identify_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        dict {label: list of region indices}\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        sys.stderr.write('Regions on section %d are not identified because the section is invalid.\\n' % sec)\n",
    "        return\n",
    "    \n",
    "    region_contours = load_cell_classifier_data(what='region_contours', stack=stack, sec=sec, ext='bp')\n",
    "    \n",
    "    surround_margins = set([get_margin_from_surround_label(label) for label in labels_to_sample if is_surround_label(label)])\n",
    "    region_labels = label_regions(stack=stack, section=sec, \n",
    "                                  region_contours=region_contours,\n",
    "                                  surround_margins=surround_margins,\n",
    "                                  labeled_contours=labeled_contours)\n",
    "    region_labels = {label: region_indices for label, region_indices in region_labels.iteritems()\n",
    "                     if len(region_indices) > 0}\n",
    "    return region_labels\n",
    "\n",
    "def sample_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours, num_samples_per_label=None):\n",
    "    \"\"\"\n",
    "    Sample region addresses in a particular section and associate them with different labels (positive, surround, negative, noclass, foreground, background, etc.).\n",
    "    \n",
    "    Args:\n",
    "        region_contours (list of nx2 arrays): list of contour vertices.\n",
    "        labeled_contours (dict of nx2 arrays): {label: contour vertices}\n",
    "        \n",
    "    Returns:\n",
    "        dict of 3-tuple list: {label: list of (stack, section, region_index)}.\n",
    "        If section is invalid, return None.\n",
    "    \"\"\"\n",
    "\n",
    "    addresses = {}\n",
    "    \n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        sys.stderr.write('Regions on section %d are not sampled because the section is invalid.\\n' % sec)\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        region_labels = load_hdf_v2(DataManager.get_region_labels_filepath(stack=stack, sec=sec))\n",
    "        region_labels = {label: region_indices for label, region_indices in region_labels.iteritems()\n",
    "                        if label in labels_to_sample}\n",
    "    except Exception as e:\n",
    "        sys.stderr.write('%s\\n' % e)\n",
    "        sys.stderr.write('Cannot load pre-computed region labels for section %d... re-compute\\n' % sec)\n",
    "        region_labels = identify_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours)\n",
    "\n",
    "    for label, region_indices in region_labels.iteritems():\n",
    "        if num_samples_per_label is None:\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in region_indices]\n",
    "        else:\n",
    "            sampled_region_indices = np.random.choice(region_indices, min(num_samples_per_label, len(region_indices)), replace=False)\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in sampled_region_indices]\n",
    "\n",
    "    return addresses\n",
    "\n",
    "def load_cell_based_features_one_section_(stack, section, region_indices, cell_features_used):\n",
    "    \"\"\"\n",
    "    Load pre-computed cell-based features for a list of regions on a particular section.\n",
    "    \"\"\"\n",
    "    \n",
    "    region_features_all_regions = load_cell_classifier_data(what='region_features', stack=stack, sec=section, ext='hdf')\n",
    "    # Loading hdf ~ 2 seconds.\n",
    "    \n",
    "    all_features_normalized = []\n",
    "    for feature_name in cell_features_used:\n",
    "        feats = np.asarray([region_features_all_regions[region_ind][feature_name] for region_ind in region_indices])\n",
    "        \n",
    "        # huMomentsHist feats: n_regions x 7 x n_bins\n",
    "        # other feats: n_regions x n_bins\n",
    "        \n",
    "        feats_normalized = feats/feats.sum(axis=-1)[...,None].astype(np.float)\n",
    "        if feature_name == 'huMomentsHist':\n",
    "            feats_normalized = feats_normalized.reshape((feats_normalized.shape[0], -1))\n",
    "        all_features_normalized.append(feats_normalized)\n",
    "\n",
    "    features = np.hstack(all_features_normalized)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def generate_dataset_cell_based(num_samples_per_label, stacks, labels_to_sample, cell_features_used):\n",
    "    \"\"\"\n",
    "    Generate dataset.\n",
    "    - Extract addresses\n",
    "    - Map addresses to features\n",
    "    - Remove None features\n",
    "    \n",
    "    Returns:\n",
    "        features, addresseslab\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample addresses\n",
    "    \n",
    "    addresses = defaultdict(list)\n",
    "    addresses_by_section_by_label = []\n",
    "    \n",
    "#     t = time.time()\n",
    "    \n",
    "    for stack in stacks:\n",
    "        \n",
    "        first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    \n",
    "        t1 = time.time()\n",
    "\n",
    "        contours_df, _ = DataManager.load_annotation_v3(stack=stack)\n",
    "        labeled_contours = contours_df[(contours_df['orientation'] == 'sagittal') & (contours_df['downsample'] == 1)].drop_duplicates(subset=['section', 'name', 'side', 'filename', 'downsample', 'creator'])\n",
    "        labeled_contours = convert_annotation_v3_original_to_aligned_cropped(labeled_contours, stack=stack)\n",
    "\n",
    "        sys.stderr.write('Load annotation. Time: %.2f seconds.\\n' % (time.time() - t1))\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # Sample addresses from each section\n",
    "\n",
    "        # If region labels are already computed, sequential loading is much faster (3s) than parallel loading (50s) due to disk read contention.\n",
    "        addresses_by_section_by_label_curr_stack = [\n",
    "            sample_predefined_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "                                                                 labels_to_sample=labels_to_sample,\n",
    "                                                                 labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "                                                                 num_samples_per_label=30)\n",
    "            for sec in range(first_sec, last_sec+1)]\n",
    "\n",
    "#         pool = Pool(NUM_CORES/2)\n",
    "#         addresses_by_section_by_label_curr_stack = \\\n",
    "#         pool.map(lambda sec: sample_predefined_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "#                                                                  labels_to_sample=labels_to_sample,\n",
    "#                                                                  labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "#                                                                  num_samples_per_label=30),\n",
    "#                  range(first_sec, last_sec+1))\n",
    "#         pool.close()\n",
    "#         pool.join()\n",
    "                \n",
    "        addresses_by_section_by_label += addresses_by_section_by_label_curr_stack\n",
    "\n",
    "        sys.stderr.write('Sample addresses (stack %s): %.2s seconds.\\n' % (stack, time.time() - t1))\n",
    "        \n",
    "    # Aggregate addresses sampled form each section\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    addresses_by_label = defaultdict(list)\n",
    "    for addrs_by_label in addresses_by_section_by_label:\n",
    "        if addrs_by_label is not None: # handle cases of invalid sections for which sampling function returns None\n",
    "            for label, addrs in addrs_by_label.iteritems():\n",
    "                addresses_by_label[label] += addrs\n",
    "    addresses_by_label.default_factory = None\n",
    "    \n",
    "    if num_samples_per_label is not None:\n",
    "        import random\n",
    "        addresses_by_label = {label: random.sample(addrs, min(num_samples_per_label/len(stacks), len(addrs))) \n",
    "                              for label, addrs in addresses_by_label.iteritems()}\n",
    "\n",
    "    # Remove unwanted labels\n",
    "    addresses_by_label = {label: addrs for label, addrs in addresses_by_label.iteritems() if label in labels_to_sample}\n",
    "    \n",
    "    sys.stderr.write('Aggregate addresses: %.2f seconds\\n' % (time.time() - t))    \n",
    "    \n",
    "    # Map addresses to features\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    load_cell_based_features_given_address_list = lambda addrs: smart_map(addrs, keyfunc=lambda (st, se, ri): (st, se),\\\n",
    "                        func=lambda (st, se), gr: \\\n",
    "                        load_cell_based_features_one_section_(stack=st, section=se, \n",
    "                                                              region_indices=[ri for _,_,ri in gr], cell_features_used=cell_features_used))\n",
    "    features_by_label = apply_function_to_dict(load_cell_based_features_given_address_list, addresses_by_label)\n",
    "    features_by_label = apply_function_to_dict(np.asarray, features_by_label)\n",
    "    \n",
    "    sys.stderr.write('Map addresses to features: %.2f seconds\\n' % (time.time() - t))\n",
    "    \n",
    "    # Remove features that are None are contain nan values.\n",
    "\n",
    "    for name in features_by_label.keys():\n",
    "        valid = [(ftr, addr) for ftr, addr in zip(features_by_label[name], addresses_by_label[name])\n",
    "                    if ftr is not None and not np.any(np.isnan(ftr))]\n",
    "        res = zip(*valid)\n",
    "        features_by_label[name] = np.array(res[0])\n",
    "        addresses_by_label[name] = res[1]\n",
    "    \n",
    "    return features_by_label, addresses_by_label\n",
    "\n",
    "# Generate training data set\n",
    "features, addresses = generate_dataset_cell_based(num_samples_per_label=num_samples_per_label, \n",
    "                                                  stacks=stacks,\n",
    "                                                  labels_to_sample=labels_to_sample,\n",
    "                                                 cell_features_used=cell_features_used)\n",
    "# Save training data set to '/shared/CSHL_cells_v2/classifiers/datasets/'\n",
    "features_fp = os.path.join(CELL_FEATURES_CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_features.hdf')\n",
    "create_parent_dir_if_not_exists(features_fp)\n",
    "save_hdf_v2(features, features_fp)\n",
    "upload_to_s3(features_fp)\n",
    "# train_feat_dir = create_if_not_exists(os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset, 'patch_features'))\n",
    "# for label, feats in training_features.iteritems():\n",
    "#     bp.pack_ndarray_file(feats, os.path.join(train_feat_dir, label + '.bp'))\n",
    "\n",
    "    # Save addresses\n",
    "addresses_fp = os.path.join(CELL_FEATURES_CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_addresses.pkl')\n",
    "save_pickle(addresses, addresses_fp)\n",
    "upload_to_s3(addresses_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features['10N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('MD589', 210, 36796),\n",
       " ('MD589', 229, 41148),\n",
       " ('MD589', 210, 36467),\n",
       " ('MD589', 211, 36857),\n",
       " ('MD589', 210, 37448),\n",
       " ('MD589', 211, 37524),\n",
       " ('MD589', 232, 39643),\n",
       " ('MD589', 232, 40260),\n",
       " ('MD589', 229, 41010),\n",
       " ('MD589', 210, 37121))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addresses['10N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10N 10\n",
      "10N_surround_500_12N 952\n",
      "10N_surround_500_noclass 998\n",
      "12N 1000\n",
      "12N_surround_500_10N 10\n",
      "12N_surround_500_AP 11\n",
      "12N_surround_500_noclass 969\n",
      "3N 689\n",
      "3N_surround_500_noclass 680\n",
      "4N_surround_500_3N 91\n",
      "4N_surround_500_noclass 335\n",
      "5N 1000\n",
      "5N_surround_500_7n 13\n",
      "5N_surround_500_noclass 992\n",
      "6N 23\n",
      "6N_surround_500_7n 8\n",
      "6N_surround_500_noclass 148\n",
      "7N 1000\n",
      "7N_surround_500_noclass 999\n",
      "7n 388\n",
      "7n_surround_500_5N 2\n",
      "7n_surround_500_6N 5\n",
      "7n_surround_500_noclass 960\n",
      "AP 333\n",
      "AP_surround_500_12N 18\n",
      "AP_surround_500_noclass 423\n",
      "Amb 205\n",
      "Amb_surround_500_7N 3\n",
      "Amb_surround_500_noclass 390\n",
      "DC 1000\n",
      "DC_surround_500_VCA 898\n",
      "DC_surround_500_VCP 994\n",
      "DC_surround_500_noclass 934\n",
      "IC 1000\n",
      "IC_surround_500_SC 1000\n",
      "IC_surround_500_noclass 921\n",
      "LC 713\n",
      "LC_surround_500_noclass 715\n",
      "LRt 1000\n",
      "LRt_surround_500_Sp5C 130\n",
      "LRt_surround_500_noclass 999\n",
      "PBG 259\n",
      "PBG_surround_500_SNR 17\n",
      "PBG_surround_500_VLL 98\n",
      "PBG_surround_500_noclass 777\n",
      "Pn 933\n",
      "Pn_surround_500_RtTg 178\n",
      "Pn_surround_500_VLL 24\n",
      "Pn_surround_500_noclass 898\n",
      "RMC 960\n",
      "RMC_surround_500_noclass 958\n",
      "RtTg 390\n",
      "RtTg_surround_500_Pn 224\n",
      "RtTg_surround_500_Tz 49\n",
      "RtTg_surround_500_noclass 394\n",
      "SC 1000\n",
      "SC_surround_500_IC 1000\n",
      "SC_surround_500_noclass 983\n",
      "SNC 571\n",
      "SNC_surround_500_SNR 987\n",
      "SNC_surround_500_noclass 987\n",
      "SNR 940\n",
      "SNR_surround_500_PBG 3\n",
      "SNR_surround_500_SNC 571\n",
      "SNR_surround_500_VLL 26\n",
      "SNR_surround_500_noclass 937\n",
      "Sp5C 998\n",
      "Sp5C_surround_500_LRt 18\n",
      "Sp5C_surround_500_Sp5I 630\n",
      "Sp5C_surround_500_noclass 992\n",
      "Sp5I 1000\n",
      "Sp5I_surround_500_Sp5C 630\n",
      "Sp5I_surround_500_Sp5O 1000\n",
      "Sp5I_surround_500_noclass 935\n",
      "Sp5O 987\n",
      "Sp5O_surround_500_Sp5I 999\n",
      "Sp5O_surround_500_noclass 889\n",
      "Tz 1000\n",
      "Tz_surround_500_RtTg 100\n",
      "Tz_surround_500_noclass 974\n",
      "VCA 1000\n",
      "VCA_surround_500_DC 878\n",
      "VCA_surround_500_VCP 1000\n",
      "VCA_surround_500_noclass 912\n",
      "VCP 991\n",
      "VCP_surround_500_DC 1000\n",
      "VCP_surround_500_VCA 972\n",
      "VCP_surround_500_noclass 902\n",
      "VLL 989\n",
      "VLL_surround_500_PBG 21\n",
      "VLL_surround_500_SNR 15\n",
      "VLL_surround_500_noclass 938\n",
      "bg 707\n"
     ]
    }
   ],
   "source": [
    "for l, v in sorted(addresses.items()):\n",
    "    print l, len(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
