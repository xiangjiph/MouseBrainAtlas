{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD653/MD653_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD653/MD653_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD653/MD653_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD653/MD653_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD652/MD652_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD652/MD652_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD652/MD652_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD652/MD652_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD642/MD642_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD642/MD642_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD642/MD642_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD642/MD642_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD657/MD657_cropbox.txt\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.path import Path\n",
    "from multiprocess import Pool\n",
    "import networkx as nx\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "from data_manager import *\n",
    "from metadata import *\n",
    "from cell_utilities import *\n",
    "from learning_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_radial = 4\n",
    "# radial_bins = np.logspace(0, 2, 10, base=10)\n",
    "radial_bins = np.linspace(0, 100, n_radial+1)\n",
    "\n",
    "n_angular = 8\n",
    "angular_bins = np.linspace(-np.pi, np.pi, n_angular+1)\n",
    "\n",
    "n_orientation_bins = 10\n",
    "orientation_bins = np.linspace(-np.pi/2, np.pi/2, n_orientation_bins+1)\n",
    "\n",
    "size_histogram_bins = np.r_[np.linspace(0, 3000, 10), np.inf]\n",
    "\n",
    "n_edge_length_bins = 10\n",
    "edge_length_bins = np.r_[np.linspace(0, 100, n_edge_length_bins), np.inf]\n",
    "\n",
    "n_edge_direction_bins = 10\n",
    "edge_direction_bins = np.linspace(-np.pi/2, np.pi/2, n_edge_direction_bins+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def compute_features_regions(stack, sec, region_contours):\n",
    "#     \"\"\"\n",
    "#     Compute features for given regions. Save to disk.\n",
    "\n",
    "#     Args:\n",
    "#         region_contours:\n",
    "#             list of n x 2 arrays.\n",
    "#     \"\"\"\n",
    "\n",
    "#     sys.stderr.write('Processing stack %s, section %d...\\n' % (stack, sec))\n",
    "\n",
    "#     cell_orientations = load_cell_data('orientation', stack, sec)\n",
    "#     cell_orientations = np.array(map(normalize_angle, cell_orientations))\n",
    "\n",
    "#     cell_centroids = load_cell_data('centroid', stack, sec)\n",
    "#     cell_numbers = cell_centroids.shape[0]\n",
    "\n",
    "#     cells = load_cell_data('cells_aligned_mirrored_padded', stack=stack, sec=sec, ext='bp')\n",
    "#     cells_flattened = cells.reshape((cells.shape[0], -1))\n",
    "    \n",
    "#     cell_sizes = load_cell_data('cellSizes', stack=stack, sec=sec, ext='bp')\n",
    "\n",
    "#     large_cell_indices = load_cell_data('largeCellIndices', stack=stack, sec=sec, ext='bp')\n",
    "#     large_cell_centroids = cell_centroids[large_cell_indices]\n",
    "    \n",
    "#     small_cell_indices = np.setdiff1d(range(cell_numbers), large_cell_indices)\n",
    "#     small_cell_centroids = cell_centroids[small_cell_indices]\n",
    "    \n",
    "#     large_cell_moments = load_cell_data('largeCellFeatures', stack=stack, sec=sec, ext='bp')\n",
    "    \n",
    "#     large_cell_neighbor_indices = load_cell_data('neighborCellIndices', stack=stack, sec=sec, ext='hdf')\n",
    "#     neighbors = dict(zip(large_cell_indices, large_cell_neighbor_indices))\n",
    "    \n",
    "#     large_cell_neighbor_vectors = load_cell_data('neighborVectors', stack=stack, sec=sec, ext='hdf')\n",
    "#     neighbor_vectors = dict(zip(large_cell_indices, large_cell_neighbor_vectors))\n",
    "    \n",
    "#     def compute_features_one_region(cnt):\n",
    "        \n",
    "#         region_feature = {}\n",
    "\n",
    "#         # Get large cells inside the particular contour\n",
    "#         large_cell_is_inside = Path(cnt.astype(np.int)).contains_points(large_cell_centroids)\n",
    "#         large_cell_indices_inside = large_cell_indices[large_cell_is_inside]\n",
    "# #         print '%d large cells are identified in %s.' % (len(large_cell_indices_inside), name_u)\n",
    "\n",
    "#         # Get small cells inside the particular contour\n",
    "#         small_cell_is_inside = Path(cnt.astype(np.int)).contains_points(small_cell_centroids)\n",
    "#         small_cell_indices_inside = small_cell_indices[small_cell_is_inside]\n",
    "# #         print '%d small cells are identified in %s.' % (len(small_cell_indices_inside), name_u)\n",
    "\n",
    "#         ############################################\n",
    "#         ## Compute shape descriptor distribution  ##\n",
    "#         ############################################\n",
    "\n",
    "#         large_cell_moments_inside_histograms = \\\n",
    "#         np.array([np.histogram(large_cell_moments[large_cell_is_inside, i], bins=10)[0]\n",
    "#                   for i in range(7)])\n",
    "        \n",
    "#         region_feature['huMomentsHist'] = large_cell_moments_inside_histograms\n",
    "        \n",
    "#         ##################################\n",
    "#         # Compute cell size distribution #\n",
    "#         ##################################\n",
    "\n",
    "#         region_feature['largeSizeHist'] = np.histogram(cell_sizes[large_cell_indices_inside], \n",
    "#                                                            bins=size_histogram_bins)[0]\n",
    "#         all_cells_indices_inside = np.r_[large_cell_indices_inside, small_cell_indices_inside]\n",
    "        \n",
    "#         region_feature['allSizeHist'] = np.histogram(cell_sizes[all_cells_indices_inside],\n",
    "#                                                          bins=size_histogram_bins)[0]\n",
    "#         region_feature['largeSizeWeightedHist'] = np.histogram(cell_sizes[all_cells_indices_inside],\n",
    "#                                                               bins=size_histogram_bins,\n",
    "#                                                               weights=cell_sizes[all_cells_indices_inside])[0]\n",
    "        \n",
    "#         #########################################\n",
    "#         # Compute cell orientation distribution #\n",
    "#         #########################################\n",
    "\n",
    "#         region_feature['largeOrientationHist'] = np.histogram(cell_orientations[large_cell_indices_inside], \n",
    "#                                                                   bins=orientation_bins)[0]\n",
    "\n",
    "#         ##########################################\n",
    "#         # Construct large-all graph\n",
    "#         # Start from large cells, link to all cells\n",
    "#         ##########################################\n",
    "\n",
    "#         large_all_graph = nx.Graph()\n",
    "\n",
    "#         for source_sectionwise_idx in large_cell_indices_inside:\n",
    "            \n",
    "#             try:\n",
    "#                 nbrs = neighbors[source_sectionwise_idx]\n",
    "#             except:\n",
    "#                 print source_sectionwise_idx, stack, sec\n",
    "            \n",
    "#             if len(nbrs) == 0:\n",
    "#                 continue\n",
    "\n",
    "#             neighbor_cells_flattened = cells_flattened[nbrs]\n",
    "#             jacs = compute_jaccard_x_vs_list_v2(cells_flattened[source_sectionwise_idx], neighbor_cells_flattened)\n",
    "\n",
    "#             for i_sectionwise_idx, vec, jac in zip(nbrs, neighbor_vectors[source_sectionwise_idx], jacs):\n",
    "#                 length = np.sqrt(np.sum(vec**2))\n",
    "#                 direction = np.arctan2(vec[1], vec[0])\n",
    "\n",
    "#                 # Force into between -np.pi/2 and np.pi/2\n",
    "#                 if direction > np.pi/2:\n",
    "#                     direction = direction - np.pi\n",
    "#                 elif direction < -np.pi/2:\n",
    "#                     direction = direction + np.pi\n",
    "\n",
    "#                 size_diff = np.abs(cell_sizes[source_sectionwise_idx] - cell_sizes[i_sectionwise_idx])\n",
    "\n",
    "#                 orientation_diff = cell_orientations[source_sectionwise_idx] - cell_orientations[i_sectionwise_idx]\n",
    "#                 if orientation_diff > np.pi/2:\n",
    "#                     orientation_diff -= np.pi\n",
    "#                 elif orientation_diff < -np.pi/2:\n",
    "#                     orientation_diff += np.pi\n",
    "\n",
    "#                 large_all_graph.add_edge(source_sectionwise_idx, i_sectionwise_idx, weight=1, length=length, direction=direction,\n",
    "#                            orientation_diff=orientation_diff,\n",
    "#                           size_diff=size_diff,\n",
    "#                           jaccard=jac)\n",
    "\n",
    "#         ###########################\n",
    "#         # Collect edge attributes # \n",
    "#         ###########################\n",
    "\n",
    "# #         all_edge_length = []\n",
    "# #         all_edge_direction = []\n",
    "# #         for u, v in g.edges_iter():\n",
    "# #         #     print u, v\n",
    "# #             ed = g.get_edge_data(u, v)\n",
    "# #             all_edge_length.append(ed['length'])\n",
    "# #             all_edge_direction.append(ed['direction'])\n",
    "\n",
    "#         #######################################\n",
    "#         # Compute neighbor distance histogram #\n",
    "#         #######################################\n",
    "\n",
    "#     #     print 'min', np.min(all_edge_length), 'max', np.max(all_edge_length)\n",
    "\n",
    "# #         edge_length_histogram, _ = np.histogram(all_edge_length, bins=edge_length_bins)\n",
    "# #         neighbor_distance_histogram_all_regions.append(edge_length_histogram)\n",
    "\n",
    "#         ########################################\n",
    "#         # Compute neighbor direction histogram #\n",
    "#         ########################################\n",
    "\n",
    "# #         edge_direction_histogram, _ = np.histogram(all_edge_direction, bins=edge_direction_bins)\n",
    "# #         neighbor_direction_histogram_all_regions.append(edge_direction_histogram)\n",
    "        \n",
    "#         #################################\n",
    "#         # Subgraph between large cells. #\n",
    "#         #################################\n",
    "\n",
    "#         large_large_subgraph = large_all_graph.subgraph(large_cell_indices_inside)\n",
    "\n",
    "#         all_large_cell_graph_edge_length = []\n",
    "#         all_large_cell_graph_edge_direction = []\n",
    "#         all_large_large_graph_edge_jaccard = []\n",
    "#         all_large_large_graph_edge_sizeDiff = []\n",
    "#         all_large_large_graph_edge_orientationDiff = []\n",
    "        \n",
    "#         for u, v in large_large_subgraph.edges_iter():\n",
    "#             edge_data = large_large_subgraph.get_edge_data(u, v)\n",
    "#             all_large_cell_graph_edge_length.append(edge_data['length'])\n",
    "#             all_large_cell_graph_edge_direction.append(edge_data['direction'])\n",
    "#             all_large_large_graph_edge_jaccard.append(edge_data['jaccard'])\n",
    "#             all_large_large_graph_edge_sizeDiff.append(edge_data['size_diff'])\n",
    "#             all_large_large_graph_edge_orientationDiff.append(edge_data['orientation_diff'])\n",
    "        \n",
    "#         region_feature['largeLargeLinkJacHist'] = np.histogram(all_large_large_graph_edge_jaccard, bins=10)[0]\n",
    "#         region_feature['largeLargeLinkSizeDiffHist'] = np.histogram(all_large_large_graph_edge_sizeDiff, bins=10)[0]\n",
    "#         region_feature['largeLargeLinkOrientationDiffHist'] = np.histogram(all_large_large_graph_edge_orientationDiff, bins=10)[0]\n",
    "#         region_feature['largeLargeLinkLenHist'] = np.histogram(all_large_cell_graph_edge_length, bins=edge_length_bins)[0]\n",
    "#         region_feature['largeLargeLinkDirHist'] = np.histogram(all_large_cell_graph_edge_direction, bins=edge_direction_bins)[0]\n",
    "\n",
    "#         ###########################\n",
    "#         # Large/small cells graph #\n",
    "#         ###########################\n",
    "\n",
    "#         all_large_small_cell_graph_edge_length = []\n",
    "#         all_large_small_cell_graph_edge_direction = []\n",
    "\n",
    "#         for u, v in large_all_graph.edges_iter():\n",
    "#             if (u in large_cell_indices_inside and v in small_cell_indices_inside) or \\\n",
    "#             (v in large_cell_indices_inside and u in small_cell_indices_inside):\n",
    "#                 ed = large_all_graph.get_edge_data(u,v)\n",
    "#                 all_large_small_cell_graph_edge_length.append(ed['length'])\n",
    "#                 all_large_small_cell_graph_edge_direction.append(ed['direction'])\n",
    "\n",
    "#         region_feature['largeSmallLinkLenHist'] = np.histogram(all_large_small_cell_graph_edge_length, bins=edge_length_bins)[0]\n",
    "#         region_feature['largeSmallLinkDirHist'] = np.histogram(all_large_small_cell_graph_edge_direction, bins=edge_direction_bins)[0]\n",
    "\n",
    "#         return region_feature\n",
    "    \n",
    "# #     region_features = [compute_features_one_region(cnt) for cnt in region_contours]\n",
    "    \n",
    "#     pool = Pool(15)\n",
    "#     region_features = pool.map(compute_features_one_region, region_contours)\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "        \n",
    "#     # Saving as pickle is smaller than saving as hdf.\n",
    "    \n",
    "# #     fp = get_cell_classifier_data_filepath(what='region_features', stack=stack, sec=sec, ext='hdf')\n",
    "# #     save_hdf_v2(region_features, fp)\n",
    "#     fp = get_cell_classifier_data_filepath(what='region_features', stack=stack, sec=sec, ext='pkl')\n",
    "#     save_pickle(region_features, fp)\n",
    "    \n",
    "#     fp = get_cell_classifier_data_filepath(what='region_contours', stack=stack, sec=sec, ext='bp')\n",
    "#     create_parent_dir_if_not_exists(fp)\n",
    "#     bp.pack_ndarray_file(region_contours, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_size = 224\n",
    "half_size = patch_size/2\n",
    "patch_spacing = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 48771 regions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stack MD590, section 204...\n"
     ]
    }
   ],
   "source": [
    "# for stack in all_annotated_nissl_stacks:  \n",
    "for stack in ['MD590']:\n",
    "    \n",
    "    grid_spec = (patch_size, patch_spacing, ) + metadata_cache['image_shape'][stack]\n",
    "    grid_locations = grid_parameters_to_sample_locations(grid_spec)\n",
    "\n",
    "    first_section, last_section = metadata_cache['section_limits'][stack]\n",
    "    \n",
    "#     for sec in range(first_section, last_section+1):\n",
    "    for sec in range(204, 205):\n",
    "\n",
    "        if is_invalid(sec=sec, stack=stack):\n",
    "            continue\n",
    "        \n",
    "        # Define regions as all patches inside a mask.\n",
    "            \n",
    "        mask_tb = DataManager.load_thumbnail_mask_v2(stack, sec)\n",
    "        region_patch_indices = locate_patches_v2(stack=stack, grid_spec=grid_spec, mask_tb=mask_tb)\n",
    "        print 'Use %s regions.' % len(region_patch_indices)\n",
    "        \n",
    "        region_patch_locations = grid_locations[region_patch_indices]\n",
    "        ul = region_patch_locations + (-half_size, -half_size)\n",
    "        ur = region_patch_locations + (-half_size, half_size)\n",
    "        lr = region_patch_locations + (half_size, half_size)\n",
    "        ll = region_patch_locations + (half_size, -half_size)\n",
    "        \n",
    "        region_contours = np.array(zip(ul, ur, lr, ll))\n",
    "        \n",
    "        t = time.time()\n",
    "#         compute_features_regions(stack, sec, region_contours[:1000])\n",
    "\n",
    "        sys.stderr.write('Processing stack %s, section %d...\\n' % (stack, sec))\n",
    "\n",
    "        cell_orientations = load_cell_data('orientation', stack, sec)\n",
    "        cell_orientations = np.array(map(normalize_angle, cell_orientations))\n",
    "\n",
    "        cell_centroids = load_cell_data('centroid', stack, sec)\n",
    "        cell_numbers = cell_centroids.shape[0]\n",
    "\n",
    "        cells = load_cell_data('cells_aligned_mirrored_padded', stack=stack, sec=sec, ext='bp')\n",
    "        cells_flattened = cells.reshape((cells.shape[0], -1))\n",
    "\n",
    "        cell_sizes = load_cell_data('cellSizes', stack=stack, sec=sec, ext='bp')\n",
    "\n",
    "        large_cell_indices = load_cell_data('largeCellIndices', stack=stack, sec=sec, ext='bp')\n",
    "        large_cell_centroids = cell_centroids[large_cell_indices]\n",
    "\n",
    "        small_cell_indices = np.setdiff1d(range(cell_numbers), large_cell_indices)\n",
    "        small_cell_centroids = cell_centroids[small_cell_indices]\n",
    "\n",
    "        large_cell_moments = load_cell_data('largeCellFeatures', stack=stack, sec=sec, ext='bp')\n",
    "\n",
    "        large_cell_neighbor_indices = load_cell_data('neighborCellIndices', stack=stack, sec=sec, ext='hdf')\n",
    "        neighbors = dict(zip(large_cell_indices, large_cell_neighbor_indices))\n",
    "\n",
    "        large_cell_neighbor_vectors = load_cell_data('neighborVectors', stack=stack, sec=sec, ext='hdf')\n",
    "        neighbor_vectors = dict(zip(large_cell_indices, large_cell_neighbor_vectors))\n",
    "\n",
    "        def compute_features_one_region(cnt):\n",
    "\n",
    "            region_feature = {}\n",
    "\n",
    "            # Get large cells inside the particular contour\n",
    "            large_cell_is_inside = Path(cnt.astype(np.int)).contains_points(large_cell_centroids)\n",
    "            large_cell_indices_inside = large_cell_indices[large_cell_is_inside]\n",
    "    #         print '%d large cells are identified in %s.' % (len(large_cell_indices_inside), name_u)\n",
    "\n",
    "            # Get small cells inside the particular contour\n",
    "            small_cell_is_inside = Path(cnt.astype(np.int)).contains_points(small_cell_centroids)\n",
    "            small_cell_indices_inside = small_cell_indices[small_cell_is_inside]\n",
    "    #         print '%d small cells are identified in %s.' % (len(small_cell_indices_inside), name_u)\n",
    "\n",
    "            ############################################\n",
    "            ## Compute shape descriptor distribution  ##\n",
    "            ############################################\n",
    "\n",
    "            large_cell_moments_inside_histograms = \\\n",
    "            np.array([np.histogram(large_cell_moments[large_cell_is_inside, i], bins=10)[0]\n",
    "                      for i in range(7)])\n",
    "\n",
    "            region_feature['huMomentsHist'] = large_cell_moments_inside_histograms\n",
    "\n",
    "            ##################################\n",
    "            # Compute cell size distribution #\n",
    "            ##################################\n",
    "\n",
    "            region_feature['largeSizeHist'] = np.histogram(cell_sizes[large_cell_indices_inside], \n",
    "                                                               bins=size_histogram_bins)[0]\n",
    "            all_cells_indices_inside = np.r_[large_cell_indices_inside, small_cell_indices_inside]\n",
    "\n",
    "            region_feature['allSizeHist'] = np.histogram(cell_sizes[all_cells_indices_inside],\n",
    "                                                             bins=size_histogram_bins)[0]\n",
    "            region_feature['largeSizeWeightedHist'] = np.histogram(cell_sizes[all_cells_indices_inside],\n",
    "                                                                  bins=size_histogram_bins,\n",
    "                                                                  weights=cell_sizes[all_cells_indices_inside])[0]\n",
    "\n",
    "            #########################################\n",
    "            # Compute cell orientation distribution #\n",
    "            #########################################\n",
    "\n",
    "            region_feature['largeOrientationHist'] = np.histogram(cell_orientations[large_cell_indices_inside], \n",
    "                                                                      bins=orientation_bins)[0]\n",
    "\n",
    "            ##########################################\n",
    "            # Construct large-all graph\n",
    "            # Start from large cells, link to all cells\n",
    "            ##########################################\n",
    "\n",
    "            large_all_graph = nx.Graph()\n",
    "\n",
    "            for source_sectionwise_idx in large_cell_indices_inside:\n",
    "\n",
    "                try:\n",
    "                    nbrs = neighbors[source_sectionwise_idx]\n",
    "                except:\n",
    "                    print source_sectionwise_idx, stack, sec\n",
    "\n",
    "                if len(nbrs) == 0:\n",
    "                    continue\n",
    "\n",
    "                neighbor_cells_flattened = cells_flattened[nbrs]\n",
    "                jacs = compute_jaccard_x_vs_list_v2(cells_flattened[source_sectionwise_idx], neighbor_cells_flattened)\n",
    "\n",
    "                for i_sectionwise_idx, vec, jac in zip(nbrs, neighbor_vectors[source_sectionwise_idx], jacs):\n",
    "                    length = np.sqrt(np.sum(vec**2))\n",
    "                    direction = np.arctan2(vec[1], vec[0])\n",
    "\n",
    "                    # Force into between -np.pi/2 and np.pi/2\n",
    "                    if direction > np.pi/2:\n",
    "                        direction = direction - np.pi\n",
    "                    elif direction < -np.pi/2:\n",
    "                        direction = direction + np.pi\n",
    "\n",
    "                    size_diff = np.abs(cell_sizes[source_sectionwise_idx] - cell_sizes[i_sectionwise_idx])\n",
    "\n",
    "                    orientation_diff = cell_orientations[source_sectionwise_idx] - cell_orientations[i_sectionwise_idx]\n",
    "                    if orientation_diff > np.pi/2:\n",
    "                        orientation_diff -= np.pi\n",
    "                    elif orientation_diff < -np.pi/2:\n",
    "                        orientation_diff += np.pi\n",
    "\n",
    "                    large_all_graph.add_edge(source_sectionwise_idx, i_sectionwise_idx, weight=1, length=length, direction=direction,\n",
    "                               orientation_diff=orientation_diff,\n",
    "                              size_diff=size_diff,\n",
    "                              jaccard=jac)\n",
    "\n",
    "            ###########################\n",
    "            # Collect edge attributes # \n",
    "            ###########################\n",
    "\n",
    "    #         all_edge_length = []\n",
    "    #         all_edge_direction = []\n",
    "    #         for u, v in g.edges_iter():\n",
    "    #         #     print u, v\n",
    "    #             ed = g.get_edge_data(u, v)\n",
    "    #             all_edge_length.append(ed['length'])\n",
    "    #             all_edge_direction.append(ed['direction'])\n",
    "\n",
    "            #######################################\n",
    "            # Compute neighbor distance histogram #\n",
    "            #######################################\n",
    "\n",
    "        #     print 'min', np.min(all_edge_length), 'max', np.max(all_edge_length)\n",
    "\n",
    "    #         edge_length_histogram, _ = np.histogram(all_edge_length, bins=edge_length_bins)\n",
    "    #         neighbor_distance_histogram_all_regions.append(edge_length_histogram)\n",
    "\n",
    "            ########################################\n",
    "            # Compute neighbor direction histogram #\n",
    "            ########################################\n",
    "\n",
    "    #         edge_direction_histogram, _ = np.histogram(all_edge_direction, bins=edge_direction_bins)\n",
    "    #         neighbor_direction_histogram_all_regions.append(edge_direction_histogram)\n",
    "\n",
    "            #################################\n",
    "            # Subgraph between large cells. #\n",
    "            #################################\n",
    "\n",
    "            large_large_subgraph = large_all_graph.subgraph(large_cell_indices_inside)\n",
    "\n",
    "            all_large_cell_graph_edge_length = []\n",
    "            all_large_cell_graph_edge_direction = []\n",
    "            all_large_large_graph_edge_jaccard = []\n",
    "            all_large_large_graph_edge_sizeDiff = []\n",
    "            all_large_large_graph_edge_orientationDiff = []\n",
    "\n",
    "            for u, v in large_large_subgraph.edges_iter():\n",
    "                edge_data = large_large_subgraph.get_edge_data(u, v)\n",
    "                all_large_cell_graph_edge_length.append(edge_data['length'])\n",
    "                all_large_cell_graph_edge_direction.append(edge_data['direction'])\n",
    "                all_large_large_graph_edge_jaccard.append(edge_data['jaccard'])\n",
    "                all_large_large_graph_edge_sizeDiff.append(edge_data['size_diff'])\n",
    "                all_large_large_graph_edge_orientationDiff.append(edge_data['orientation_diff'])\n",
    "\n",
    "            region_feature['largeLargeLinkJacHist'] = np.histogram(all_large_large_graph_edge_jaccard, bins=10)[0]\n",
    "            region_feature['largeLargeLinkSizeDiffHist'] = np.histogram(all_large_large_graph_edge_sizeDiff, bins=10)[0]\n",
    "            region_feature['largeLargeLinkOrientationDiffHist'] = np.histogram(all_large_large_graph_edge_orientationDiff, bins=10)[0]\n",
    "            region_feature['largeLargeLinkLenHist'] = np.histogram(all_large_cell_graph_edge_length, bins=edge_length_bins)[0]\n",
    "            region_feature['largeLargeLinkDirHist'] = np.histogram(all_large_cell_graph_edge_direction, bins=edge_direction_bins)[0]\n",
    "\n",
    "            ###########################\n",
    "            # Large/small cells graph #\n",
    "            ###########################\n",
    "\n",
    "            all_large_small_cell_graph_edge_length = []\n",
    "            all_large_small_cell_graph_edge_direction = []\n",
    "\n",
    "            for u, v in large_all_graph.edges_iter():\n",
    "                if (u in large_cell_indices_inside and v in small_cell_indices_inside) or \\\n",
    "                (v in large_cell_indices_inside and u in small_cell_indices_inside):\n",
    "                    ed = large_all_graph.get_edge_data(u,v)\n",
    "                    all_large_small_cell_graph_edge_length.append(ed['length'])\n",
    "                    all_large_small_cell_graph_edge_direction.append(ed['direction'])\n",
    "\n",
    "            region_feature['largeSmallLinkLenHist'] = np.histogram(all_large_small_cell_graph_edge_length, bins=edge_length_bins)[0]\n",
    "            region_feature['largeSmallLinkDirHist'] = np.histogram(all_large_small_cell_graph_edge_direction, bins=edge_direction_bins)[0]\n",
    "\n",
    "            return region_feature\n",
    "\n",
    "#         region_features = [compute_features_one_region(cnt) for cnt in region_contours]\n",
    "\n",
    "        pool = Pool(15)\n",
    "        region_features = pool.map(compute_features_one_region, region_contours)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "#         # Compared to saving as pkl, saving as hdf results in both smaller file and 20x faster loading.\n",
    "#         fp = get_cell_classifier_data_filepath(what='region_features', stack=stack, sec=sec, ext='hdf')\n",
    "#         create_parent_dir_if_not_exists(fp)\n",
    "#         save_hdf_v2(region_features, fp)\n",
    "        \n",
    "#         fp = get_cell_classifier_data_filepath(what='region_contours', stack=stack, sec=sec, ext='bp')\n",
    "#         bp.pack_ndarray_file(region_contours, fp)\n",
    "\n",
    "#         del region_features\n",
    "        \n",
    "#         sys.stderr.write('Time: %d seconds\\n' % (time.time()-t)) # 100 seconds / 46k regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9738a9d35a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_hdf_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_cell_classifier_data_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'region_features'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stack' is not defined"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "q = load_hdf_v2(get_cell_classifier_data_filepath(what='region_features', stack=stack, sec=sec, ext='hdf'))\n",
    "print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
